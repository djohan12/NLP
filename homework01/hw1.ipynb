{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1740081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "from utils import Vocab, read_data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # to use gpu on kaggle or colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89395b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1-Gram Model:\n",
      "0.17344519423673171 0.17463940113200657\n",
      "<BOS>\"I'm not ready to go,\" said                                                                                                    \n",
      "<BOS>Lily and Max were best friends. One day                                                                                                    \n",
      "<BOS>He picked up the juice and                                                                                                    \n",
      "<BOS>It was raining, so                                                                                                    \n",
      "<BOS>The end of the story was                                                                                                    \n",
      "\n",
      "5-Gram Model:\n",
      "0.5794273208097757 0.5728501004199379\n",
      "<BOS>\"I'm not ready to go,\" said, \"i wanted to the boy who listen the boy who listen the boy who listen the boy who listen the boy w\n",
      "<BOS>Lily and Max were best friends. One day, the boy who listen the boy who listen the boy who listen the boy who listen the boy who listen the\n",
      "<BOS>He picked up the juice and said, \"i wanted to the boy who listen the boy who listen the boy who listen the boy who listen the \n",
      "<BOS>It was raining, so happy and said, \"i wanted to the boy who listen the boy who listen the boy who listen the boy who l\n",
      "<BOS>The end of the story was a little girl named lily was a little girl named lily was a little girl named lily was a little gir\n"
     ]
    }
   ],
   "source": [
    "class NGramModel:\n",
    "\tdef __init__(self, n, data):\n",
    "\t\tself.n = n\n",
    "\t\tself.vocab = Vocab()\n",
    "\t\tself.vocab.add('<BOS>')\n",
    "\t\tself.vocab.add('<EOS>')\n",
    "\t\tself.vocab.add('<UNK>')\n",
    "\t\tfor seq in data:\n",
    "\t\t\tself.vocab.update(seq)\n",
    "\t\tself.counts = defaultdict(lambda: defaultdict(int))\n",
    "\t\tself.probs = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "\tdef start(self):\n",
    "\t\treturn ['<BOS>'] * (self.n - 1)\n",
    "\t\t\"\"\"Because we add BOS and EOS in read_data():\n",
    "\t\t\t* No extra context needed for unigram model (returns []).\n",
    "\t\t\t* n-1 extra BOS tags needed for n>1 to provide context for the first symbol.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\tdef fit(self, data):\n",
    "\t\t\"\"\"TODO: \n",
    "\t\t\t* Train the model on the training data by populating the counts. \n",
    "\t\t\t\t* For n>1, you will need to keep track of the context and keep updating it. \n",
    "\t\t\t\t* Get the starting context with self.start().\n",
    "\t\t\"\"\"\n",
    "\t\tcontext = tuple(self.start())\n",
    "\t\tfor sentence in data:\n",
    "\t\t\tfor sym in sentence:\n",
    "\t\t\t\tif sym not in self.vocab.sym2num:\n",
    "\t\t\t\t\tsym='<UNK>'\n",
    "\t\t\t\tif self.n>1:\n",
    "\t\t\t\t\tself.counts[tuple()][sym]+=1\n",
    "\t\t\t\t\tself.counts[context][sym]+=1\n",
    "\t\t\t\t\tcontext = context[1:] + (sym,)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.counts[context][sym]+=1\n",
    "\t\t\"\"\"TODO: Populate self.probs by converting counts to log probabilities with add-1 smoothing.\"\"\"\n",
    "\t\tfor context in self.counts:\n",
    "\t\t\tfor sym in self.vocab.num2sym:\n",
    "\t\t\t\tself.probs[context][sym] = math.log(self.counts[context][sym] + 1) - math.log(sum(self.counts[context].values()) + len(self.vocab))\n",
    "  \t\t\n",
    "\tdef step(self, context):\n",
    "\t\t\"\"\"Returns the distribution over possible next symbols. For unseen contexts, backs off to unigram distribution.\"\"\"\n",
    "\t\tif self.n>1:\n",
    "\t\t\tcontext=self.start()+context\n",
    "\t\t\tcontext=tuple(context[-(self.n-1):])\n",
    "\t\telse:\n",
    "\t\t\tcontext=tuple()\n",
    "\t\t\n",
    "\t\tif context in self.probs:\n",
    "\t\t\treturn self.probs[context]\n",
    "\t\telse:\n",
    "\t\t\treturn self.probs[tuple()]\n",
    "       \n",
    "\n",
    "\tdef predict(self, context):\n",
    "\t\treturn max(self.step(context).items(), key=lambda x: x[1])[0] \n",
    "\t    \n",
    "\tdef evaluate(self, data):\n",
    "\t\tcorrect, total = 0, 0\n",
    "\t\tfor sentence in data:\n",
    "\t\t\tcontext = self.start()\n",
    "   \n",
    "\t\t\tfor sym in sentence:\n",
    "\t\t\t\tpred = self.predict(context)\n",
    "\t\t\t\tif pred == sym:\n",
    "\t\t\t\t\tcorrect += 1\n",
    "\t\t\t\ttotal += 1\n",
    "\t\t\t\tif self.n > 1:\n",
    "\t\t\t\t\tcontext = context[1:] + [sym]\n",
    "\n",
    "\t\treturn correct / total if total > 0 else 0.0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\ttrain_data = read_data('data/train.txt')\n",
    "\tval_data = read_data('data/val.txt')\n",
    "\ttest_data = read_data('data/test.txt')\n",
    "\tresponse_data = read_data('data/response.txt')\n",
    "\n",
    "\tfor n in [1,5]:\n",
    "\t\tprint(f'\\n{n}-Gram Model:')\n",
    "\t\tmodel = NGramModel(n, train_data)\n",
    "\t\tmodel.fit(train_data)\n",
    "\t\tprint(f\"\\nValidation: {model.evaluate(val_data)}\\nTesting: {model.evaluate(test_data)}\\n\")\n",
    "\n",
    "\t\t\"\"\"Generate the next 100 characters for the free response questions.\"\"\"\n",
    "\t\tfor x in response_data:\n",
    "\t\t\tx = x[:-1] # remove EOS\n",
    "\t\t\tfor _ in range(100):\n",
    "\t\t\t\ty = model.predict(x)\n",
    "\t\t\t\tx += y\n",
    "\t\t\tprint(''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecceaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\tAverage Loss: 1.9258\t Time: 14.27s\n"
     ]
    }
   ],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\tdef __init__(self, vocab, dims):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.vocab = vocab\n",
    "\t\tself.dims = dims\n",
    "\t\tself.embed = nn.Embedding(len(vocab), dims)\n",
    "  \n",
    "\t\tself.Wxh = nn.Linear(dims, dims)\n",
    "\t\tself.Whh = nn.Linear(dims, dims)\n",
    "\t\tself.fc = nn.Linear(dims, len(vocab))\n",
    "\n",
    "\tdef start(self):\n",
    "\t\treturn torch.zeros(self.dims, device=device)\n",
    "\n",
    "\tdef step(self, h, idx):\n",
    "\t\t\"\"\"\tTODO: Pass idx through the layers of the model. Return the updated hidden state (h) and log probabilities.\"\"\"\n",
    "\t\temb = self.embed(torch.tensor([idx], device=device))\n",
    "\t\th_new = torch.tanh(self.Wxh(emb) + self.Whh(h.unsqueeze(0))).squeeze(0)\n",
    "\t\tlogits = self.fc(h_new)\n",
    "\t\tlog_probs = F.log_softmax(logits, dim=-1)\n",
    "\t\treturn h_new, log_probs\n",
    "\n",
    "\tdef predict(self, h, idx):\n",
    "\t\t\"\"\"\tTODO: Obtain the updated hidden state and log probabilities after calling self.step(). \n",
    "\t\t\tReturn the updated hidden state and the most likely next symbol.\"\"\"\n",
    "\t\th_new, log_probs = self.step(h,idx)\n",
    "\t\tnext_idx = torch.argmax(log_probs).item()\n",
    "\t\tnext_sym = self.vocab.denumberize(next_idx)\n",
    "\t\treturn h_new, next_sym\n",
    "\n",
    "\tdef fit(self, data, lr=0.001, epochs=10):\n",
    "\t\t\"\"\"\tTODO: Fill in the code using PyTorch functions and other functions from part2.py and utils.py.\n",
    "\t\t\tMost steps will only be 1 line of code. You may write it in the space below the step.\"\"\"\n",
    "\t\t\n",
    "\t\t# 1. Initialize the optimizer. Use `torch.optim.Adam` with `self.parameters()` and `lr`.\n",
    "\t\toptimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\t\t# 2. Set a loss function variable to `nn.NLLLoss()` for negative log-likelihood loss.\n",
    "\t\tloss_function = nn.NLLLoss()\n",
    "\t\t# 3. Loop through the specified number of epochs.\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t#\t 1. Put the model into training mode using `self.train()`.\n",
    "\t\t\tself.train()\n",
    "\t\t#\t 2. Shuffle the training data using random.shuffle().\n",
    "\t\t\trandom.shuffle(data)\n",
    "\t\t#\t 3. Initialize variables to keep track of the total loss (`total_loss`) and the total number of characters (`total_chars`).\n",
    "\t\t\ttotal_loss=0.0\n",
    "\t\t\ttotal_chars=0\n",
    "   \n",
    "\t\t\tstart_time=time.time()\n",
    "\t\t#\t 4. Loop over each sentence in the training data.\n",
    "\t\t\tfor sentence in data:\n",
    "\t\t#\t \t 1. Initialize the hidden state with the start state, move it to the proper device using `.to(device)`, and detach it from any previous computation graph with `.detach()`.\n",
    "\t\t\t\th=self.start().to(device).detach()\n",
    "\t\t#\t \t 2. Call `optimizer.zero_grad()` to clear any accumulated gradients from the previous update.\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t#\t \t 3. Initialize a variable to keep track of the loss within a sentence (`loss`).\n",
    "\t\t\t\tloss=0.0\n",
    "\t\t#\t \t 4. Loop through the characters of the sentence from position 1 to the end (i.e., start with the first real character, not BOS).\n",
    "\t\t\t\tfor i in range(1, len(sentence)):\n",
    "\t\t#\t \t \t1. You will need to keep track of the previous character (at position i-1) and current character (at position i). These should be expressed as numbers, not symbols.\n",
    "\t\t\t\t\tprev_idx = self.vocab.numberize(sentence[i-1])\n",
    "\t\t#\t\t\t2. Call self.step() to get the next hidden state and log probabilities over the vocabulary given the previous character.\n",
    "\t\t\t\t\tcurr_idx = self.vocab.numberize(sentence[i])\n",
    "\t\t#\t\t\t3. See if this matches the actual current character (numberized). Do so by computing the loss with the nn.NLLLoss() loss initialized above. \n",
    "\t\t#\t\t\t   * The first argument is the updated log probabilities returned from self.step(). You'll need to reshape it to `(1, V)` using `.view(1, -1)`.\n",
    "\t\t#\t\t\t   * The second argument is the current numberized character. It will need to be wrapped in a tensor with `device=device`. Reshape this to `(1,)` using `.view(1)`.\n",
    "\t\t\t\t\th, log_probs = self.step(h, prev_idx) \n",
    "\t\t#\t\t\t4. Add this this character loss value to `loss`.\n",
    "\t\t\t\t\tloss += loss_function(log_probs.view(1,-1), torch.tensor([curr_idx], device=device))\n",
    "\t\t#\t\t\t5. Increment `total_chars` by 1.\n",
    "\t\t\t\t\ttotal_chars+=1\n",
    "\t\t#\t \t 5. After processing the full sentence, call `loss.backward()` to compute gradients.\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t#\t\t 6. Apply gradient clipping to prevent exploding gradients. Use `torch.nn.utils.clip_grad_norm_()` with `self.parameters()` and a `max_norm` of 5.0.\n",
    "\t\t\t\ttorch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=5.0)\n",
    "\t\t#\t\t 7. Call `optimizer.step()` to update the model parameters using the computed gradients.\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t#\t\t 8. Add `loss.item()` to `total_loss`.\n",
    "\t\t\t\ttotal_loss+=loss.item()\n",
    "\t\t#\t5. Compute the average loss per character by dividing `total_loss / total_chars`.\n",
    "\t\t\taverage_loss = total_loss/total_chars\n",
    "\t\t#\t6. For debugging, it will be helpful to print the average loss per character and the runtime after each epoch. Average loss per character should always decrease epoch to epoch and drop from about 3 to 1.2 over the 10 epochs.\n",
    "\t\t\tprint(f\"Epoch {epoch+1}/{epochs}\\tAverage Loss: {average_loss:.4f}\\t Time: {time.time()-start_time:.2f}s\")\n",
    "\n",
    "\tdef evaluate(self, data):\n",
    "\t\t\"\"\"\tTODO: Iterating over the sentences in the data, calculate next character prediction accuracy. \n",
    "\t\tUse `self.eval()` and `with torch.no_grad()` so that the model is not trained during evaluation.\n",
    "\t\tUse self.predict() to get the predicted next character, and then check if it matches the real next character found in the data.\n",
    "\t\tDivide the total correct predictions by the total number of characters to get the final accuracy.\"\"\"\n",
    "\t\tself.eval()\n",
    "\t\tcorrect=0\n",
    "\t\ttotal=0\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor sentence in data:\n",
    "\t\t\t\th = self.start()\n",
    "\t\t\t\tfor i in range(1, len(sentence)):\n",
    "\t\t\t\t\tidx = self.vocab.numberize(sentence[i-1])\n",
    "\t\t\t\t\th, sym = self.predict(h, idx)\n",
    "\t\t\t\t\tif sym == sentence[i]:\n",
    "\t\t\t\t\t\tcorrect+=1\n",
    "\t\t\t\t\ttotal+=1\n",
    "\t\taccuracy = correct/total\n",
    "\t\treturn accuracy\n",
    "if __name__ == '__main__':\n",
    "\t\n",
    "\ttrain_data = read_data('data/train.txt')\n",
    "\tval_data = read_data('data/val.txt')\n",
    "\ttest_data = read_data('data/test.txt')\n",
    "\tresponse_data = read_data('data/response.txt')\n",
    "\n",
    "\tvocab = Vocab()\n",
    "\t\"\"\"TODO: Populate vocabulary with all possible characters/symbols in the training data, including '<BOS>', '<EOS>', and '<UNK>'.\"\"\"\n",
    "\n",
    "\tvocab.add('<BOS>')\n",
    "\tvocab.add('<EOS>')\n",
    "\tvocab.add('<UNK>')\n",
    "\tfor sentence in train_data:\n",
    "\t\tvocab.update(sentence)\n",
    "     \n",
    "\tmodel = RNNModel(vocab, dims=128).to(device)\n",
    "\tmodel.fit(train_data)\n",
    "\n",
    "\ttorch.save({\n",
    "\t\t'model_state_dict': model.state_dict(),\n",
    "\t\t'vocab': model.vocab,\n",
    "\t\t'dims': model.dims\n",
    "\t}, 'rnn_model.pth')\n",
    "\n",
    "\t\"\"\"Use this code if you saved the model and want to load it up again to evaluate. Comment out the model.fit() and torch.save() code if so.\n",
    "\t# checkpoint = torch.load('rnn_model.pth', map_location=device, weights_only=False)\n",
    "\t# vocab = checkpoint['vocab']\n",
    "\t# dims = checkpoint['dims']\n",
    "\t# model = RNNModel(vocab, dims).to(device)\n",
    "\t# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\t\"\"\"\n",
    "\n",
    "\tmodel.eval()\n",
    "\tprint(f\"\\nValidation: {model.evaluate(val_data)}\\nTesting: {model.evaluate(test_data)}\\n\")\n",
    "\n",
    "\t\"\"\"Generate the next 100 characters for the free response questions.\"\"\"\n",
    "\tfor x in response_data:\n",
    "\t\tx = x[:-1] # remove EOS\n",
    "\t\tstate = model.start()\n",
    "\t\tfor char in x:\n",
    "\t\t\tidx = vocab.numberize(char)\n",
    "\t\t\tstate, _ = model.step(state, idx)\n",
    "\t\tidx = vocab.numberize(x[-1])\n",
    "\t\tfor _ in range(100):\n",
    "\t\t\tstate, sym = model.predict(state, idx)\n",
    "\t\t\tx += sym # My predict() returns the denumberized symbol. Yours may work differently; change the code as needed.\n",
    "\t\tprint(''.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d64c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\tAverage Loss: 1.9433\tTime: 24.44s\n",
      "Epoch 2/10\tAverage Loss: 1.5725\tTime: 25.20s\n",
      "Epoch 3/10\tAverage Loss: 1.4469\tTime: 26.62s\n",
      "Epoch 4/10\tAverage Loss: 1.3706\tTime: 25.49s\n",
      "Epoch 5/10\tAverage Loss: 1.3168\tTime: 24.93s\n",
      "Epoch 6/10\tAverage Loss: 1.2707\tTime: 27.12s\n",
      "Epoch 7/10\tAverage Loss: 1.2341\tTime: 24.77s\n",
      "Epoch 8/10\tAverage Loss: 1.2022\tTime: 25.63s\n",
      "Epoch 9/10\tAverage Loss: 1.1740\tTime: 27.21s\n",
      "Epoch 10/10\tAverage Loss: 1.1492\tTime: 24.36s\n",
      "0.6221437859929407 0.6146550120885252\n",
      "<BOS>\"I'm not ready to go,\" saidy, the boy named timmy.<EOS> the sky and said.<EOS> the boy named timmy.<EOS>.<EOS> the boy named timmy.<EOS> the sky an\n",
      "<BOS>Lily and Max were best friends. One day.<EOS> the sky and said.<EOS> the boy named timmy.<EOS>.<EOS> the boy named timmy.<EOS> the sky and said.<EOS> the boy named\n",
      "<BOS>He picked up the juice andy the sky and said, \"i was so happy.<EOS> the sky and said.<EOS> the boy named timmy.<EOS>.<EOS> the boy named timmy\n",
      "<BOS>It was raining, son the sky and said.<EOS> the boy named timmy.<EOS>.<EOS> the boy named timmy.<EOS> the sky and said.<EOS> the boy named \n",
      "<BOS>The end of the story was on the sky.<EOS>.<EOS> the boy named timmy.<EOS>.<EOS> the boy named timmy.<EOS> the sky and said.<EOS> the boy named timmy\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab, dims):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.dims = dims\n",
    "        self.embed = nn.Embedding(len(vocab), dims)\n",
    "        self.W_x = nn.Linear(dims, 4 * dims) \n",
    "        self.W_h = nn.Linear(dims, 4 * dims, bias=False) \n",
    "        self.fc = nn.Linear(dims, len(vocab))\n",
    "\n",
    "    def start(self):\n",
    "        h = torch.zeros(1, self.dims, device=device)\n",
    "        c = torch.zeros(1, self.dims, device=device)\n",
    "        return (h, c)\n",
    "\n",
    "    def step(self, state, idx):\n",
    "        h, c = state  \n",
    "        x = self.embed(torch.tensor([idx], device=device)).unsqueeze(0)  \n",
    "        x = x.squeeze(1) \n",
    "\n",
    "        gates = self.W_x(x) + self.W_h(h) \n",
    "        i, f, g, o = gates.chunk(4, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        logits = self.fc(h_next)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "\n",
    "        return (h_next, c_next), log_probs\n",
    "\n",
    "    def predict(self, state, idx):\n",
    "        new_state, log_probs = self.step(state, idx)\n",
    "        next_idx = torch.argmax(log_probs, dim=1).item()\n",
    "        next_sym = self.vocab.denumberize(next_idx)\n",
    "        return new_state, next_sym\n",
    "\n",
    "    def fit(self, data, lr=0.001, epochs=10):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        loss_function = nn.NLLLoss()\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            random.shuffle(data)\n",
    "            total_loss = 0.0\n",
    "            total_chars = 0\n",
    "            start_time = time.time()\n",
    "            for sentence in data:\n",
    "                state = self.start()\n",
    "                state = (state[0].detach(), state[1].detach())\n",
    "                optimizer.zero_grad()\n",
    "                loss = 0.0\n",
    "                for i in range(1, len(sentence)):\n",
    "                    prev_idx = self.vocab.numberize(sentence[i - 1])\n",
    "                    curr_idx = self.vocab.numberize(sentence[i])\n",
    "                    state, log_probs = self.step(state, prev_idx)\n",
    "                    loss += loss_function(log_probs.view(1, -1), torch.tensor([curr_idx], device=device))\n",
    "                    total_chars += 1\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=5.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            avg_loss = total_loss / total_chars\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\\tAverage Loss: {avg_loss:.4f}\\tTime: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for sentence in data:\n",
    "                state = self.start()\n",
    "                for i in range(1, len(sentence)):\n",
    "                    idx = self.vocab.numberize(sentence[i - 1])\n",
    "                    state, sym = self.predict(state, idx)\n",
    "                    if sym == sentence[i]:\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "        return correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    vocab = Vocab()\n",
    "    vocab.add('<BOS>')\n",
    "    vocab.add('<EOS>')\n",
    "    vocab.add('<UNK>')\n",
    "\n",
    "    train_data = read_data('data/train.txt')\n",
    "    val_data = read_data('data/val.txt')\n",
    "    test_data = read_data('data/test.txt')\n",
    "    response_data = read_data('data/response.txt')\n",
    "\n",
    "    for sent in train_data:\n",
    "        vocab.update(sent)\n",
    "\n",
    "    model = LSTMModel(vocab, dims=128).to(device)\n",
    "    model.fit(train_data)\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab': model.vocab,\n",
    "        'dims': model.dims\n",
    "    }, 'lstm_model.pth')\n",
    "\n",
    "    model.eval()\n",
    "    print(f\"\\nValidation: {model.evaluate(val_data)}\\nTesting: {model.evaluate(test_data)}\\n\")\n",
    "\n",
    "    for x in response_data:\n",
    "        x = x[:-1]  # remove EOS\n",
    "        state = model.start()\n",
    "        for char in x:\n",
    "            idx = vocab.numberize(char)\n",
    "            state, _ = model.predict(state, idx)\n",
    "        idx = vocab.numberize(x[-1])\n",
    "        for _ in range(100):\n",
    "            state, sym = model.predict(state, idx)\n",
    "            idx = vocab.numberize(sym)\n",
    "            x += sym\n",
    "        print(''.join(x))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
