{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9UVrLjSDK3J"
      },
      "source": [
        "# NLP HW2: Parsing\n",
        "Last Updated: Sep 25 3:20 PM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nysaMbFqEIk1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n"
          ]
        }
      ],
      "source": [
        "!pip install jdc\n",
        "import jdc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jVAS8M5CKkc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: wget\n",
            "zsh:1: command not found: wget\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://raw.githubusercontent.com/aarsri/nlp_hw2/refs/heads/main/utils.py\n",
        "!wget -nc https://raw.githubusercontent.com/aarsri/nlp_hw2/refs/heads/main/trees.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTybDZltGwxw"
      },
      "source": [
        "## Part 1: Bi-LSTM POS Tagger\n",
        "In this part, you will build a part-of-speech tagger using the bidirectional LSTM architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j0OaEhyyDN6l"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /Users/johan/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "import nltk\n",
        "from utils import Vocab, read_pos_file\n",
        "from nltk.corpus import treebank\n",
        "\"\"\"You should not need any other imports.\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "nltk.download('treebank')\n",
        "brown = list(treebank.tagged_sents())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKFCXjXIHFHC"
      },
      "source": [
        "**[4 points]** Implement a **bidirectional** LSTM with embedding size 128 and hidden size 256 using torch.nn.LSTM with appropriate parameters set. Remember that PyTorch's cross entropy loss includes softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RZLxOmiHDkj0"
      },
      "outputs": [],
      "source": [
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, words, tags, embedding_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.words=words\n",
        "        self.tags=tags\n",
        "        self.words.add('<UNK>') \n",
        "        self.tags.add('<UNK>')\n",
        "\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "    \n",
        "\n",
        "        # Layers\n",
        "        self.embedding = nn.Embedding(len(self.words), embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.35)\n",
        "        self.w_out = nn.Linear(2 * hidden_dim, len(self.tags))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m4_k6KXqERUf"
      },
      "outputs": [],
      "source": [
        "%%add_to BiLSTMTagger\n",
        "def forward(self, sentence):\n",
        "\tembeddings = self.embedding(torch.tensor(sentence, dtype=torch.long))\n",
        "\tlstm_in = embeddings.view(len(sentence), 1, -1)\n",
        "\tlstm_out, _ = self.lstm(lstm_in)\n",
        "\tlstm_out = lstm_out.view(len(sentence), -1)\n",
        "\tlstm_out = self.dropout(lstm_out)\n",
        "\tscores = self.w_out(lstm_out)\n",
        "\treturn scores\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lDuGWc36E0FL"
      },
      "outputs": [],
      "source": [
        "%%add_to BiLSTMTagger\n",
        "def predict(self, scores):\n",
        "    return torch.argmax(scores, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ9dWqMWHONW"
      },
      "source": [
        "**[3 points]** Write **training** and **evaluation** procedures. These should be similar to HW1 Part 2 and 3.\n",
        "- For debugging purposes only, shorten the training data by commenting out the train_sents += brown line. Accuracy will be high because the train set is otherwise a combination of ATIS and BROWN, while the test set is ATIS. Commenting out this line will still have the model trained and tested on ATIS, yielding a high accuracy.  However, it will not be generalized well enough to do the free response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4uNfBMQxFxej"
      },
      "outputs": [],
      "source": [
        "%%add_to BiLSTMTagger\n",
        "def fit(self, data, lr=0.01, epochs=5):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        self.train()\n",
        "        random.shuffle(data)\n",
        "\n",
        "        total_loss = 0.0\n",
        "        total_tokens = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for sentence in data:\n",
        "            words = [word.lower() for word, _ in sentence]\n",
        "            tags = [tag for _, tag in sentence]\n",
        "            word_idxs = [self.words.numberize(w) for w in words]\n",
        "            inputs = torch.tensor(word_idxs, dtype=torch.long).to(next(self.parameters()).device)\n",
        "            targets = torch.tensor([self.tags.numberize(tag) for tag in tags], dtype=torch.long).to(next(self.parameters()).device)\n",
        "            self.zero_grad()\n",
        "            scores = self(inputs)   \n",
        "            loss = loss_function(scores, targets)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=5.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * len(targets)\n",
        "            total_tokens += len(targets)\n",
        "\n",
        "        average_loss = total_loss / total_tokens\n",
        "        print(f\"Epoch {epoch}/{epochs}\\tAverage Loss: {average_loss:.4f}\\tTime: {time.time() - start_time:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tlJSUs6bGROz"
      },
      "outputs": [],
      "source": [
        "%%add_to BiLSTMTagger\n",
        "def evaluate(self, data):\n",
        "    self.eval()\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentence in data:\n",
        "            words, tags = zip(*sentence)\n",
        "            word_idxs = [self.words.numberize(w.lower()) for w in words]\n",
        "            sentence = torch.tensor(word_idxs, dtype=torch.long).to(next(self.parameters()).device)\n",
        "            targets = torch.tensor([self.tags.numberize(tag) for tag in tags], dtype=torch.long).to(device)\n",
        "            scores = self(sentence)\n",
        "            predicted = self.predict(scores)\n",
        "            total_correct += sum(predicted == targets).item()\n",
        "            total_tokens += len(targets)\n",
        "    return total_correct / total_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCDNyNOjHZPm"
      },
      "source": [
        "**[1 point]** Report **loss per epoch** and **accuracy**. For full credit, accuracy must be at least 92% on val.pos and test.pos. Include your **saved model** in the submission files.\n",
        "- Because the dataset is small, scores from randomly initialized runs can have greater variance. If you're close to this score (e.g., 90%), there is likely not a bug and instead you had an unlucky run. You can run it again if you have time or comment that you believe the code is correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "a2DrvtH2GYqC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<string>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\tAverage Loss: 0.6251\tTime: 2.16s\n",
            "Epoch 2/5\tAverage Loss: 0.2728\tTime: 2.36s\n",
            "Epoch 3/5\tAverage Loss: 0.2348\tTime: 2.11s\n",
            "Epoch 4/5\tAverage Loss: 0.2054\tTime: 2.32s\n",
            "Epoch 5/5\tAverage Loss: 0.2069\tTime: 2.04s\n",
            "Validation Accuracy: 0.9346\n",
            "Test Accuracy: 0.9375\n",
            "\n",
            "Sentence 1:\n",
            "the DT DT\n",
            "flight NN NN\n",
            "should MD MD\n",
            "arrive VB VB\n",
            "at IN IN\n",
            "eleven CD CD\n",
            "a.m RB RB\n",
            "tomorrow NN NN\n",
            ". PUNC PUNC\n",
            "\n",
            "Sentence 2:\n",
            "i PRP PRP\n",
            "would MD MD\n",
            "like VB VB\n",
            "to TO TO\n",
            "find VB VB\n",
            "a DT NNP\n",
            "flight NN NN\n",
            "that WDT WDT\n",
            "goes VBZ VBZ\n",
            "from IN IN\n",
            "la NNP NNP\n",
            "guardia NNP NNP\n",
            "airport NN NNP\n",
            "to TO TO\n",
            "san NNP NNP\n",
            "jose NNP NNP\n",
            ". PUNC PUNC\n",
            "\n",
            "Sentence 3:\n",
            "show VB VB\n",
            "me PRP PRP\n",
            "the DT DT\n",
            "flights NNS NNS\n",
            "from IN IN\n",
            "newark NNP NNP\n",
            "to TO TO\n",
            "los NNP NNP\n",
            "angeles NNP NNP\n",
            ". PUNC PUNC\n",
            "\n",
            "Sentence 4:\n",
            "what WDT WP\n",
            "airline NN NN\n",
            "is VBZ VBZ\n",
            "this DT DT\n",
            "? PUNC PUNC\n",
            "\n",
            "Sentence 5:\n",
            "show VB VB\n",
            "me PRP PRP\n",
            "the DT DT\n",
            "t NNP NNP\n",
            "w NNP NNP\n",
            "a NNP NNP\n",
            "flight NN NN\n",
            ". PUNC PUNC\n",
            "\n",
            "Sentence 6:\n",
            "i PRP PRP\n",
            "would MD MD\n",
            "like VB VB\n",
            "to TO TO\n",
            "travel VB VB\n",
            "to TO TO\n",
            "westchester NNP NNP\n",
            ". PUNC PUNC\n",
            "\n",
            "Sentence 7:\n",
            "list VB VB\n",
            "american NNP NNP\n",
            "airlines NNP NNP\n",
            "flights NNS NNS\n",
            "from IN IN\n",
            "new NNP NNP\n",
            "york NNP NNP\n",
            "newark NNP NNP\n",
            "to TO TO\n",
            "nashville NNP NNP\n",
            ". PUNC PUNC\n",
            "\n",
            "Sentence 8:\n",
            "thanks UH UH\n",
            ". PUNC PUNC\n",
            "\n",
            "Sentence 9:\n",
            "what WDT WP\n",
            "flights NNS NNS\n",
            "are VBP VBP\n",
            "there EX EX\n",
            "from IN IN\n",
            "nashville NNP NNP\n",
            "to TO TO\n",
            "houston NNP NNP\n",
            "tomorrow NN NN\n",
            "evening NN NN\n",
            "that WDT WDT\n",
            "serve VBP VBP\n",
            "dinner NN NN\n",
            "? PUNC PUNC\n",
            "\n",
            "Sentence 10:\n",
            "i PRP PRP\n",
            "'d MD MD\n",
            "like VB VB\n",
            "to TO TO\n",
            "fly VB VB\n",
            "next JJ JJ\n",
            "friday NNP NNP\n",
            ". PUNC PUNC\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "    train_data = read_pos_file(\"data/train.pos\")\n",
        "    val_data = read_pos_file(\"data/val.pos\")\n",
        "    test_data = read_pos_file(\"data/test.pos\")\n",
        "    \n",
        "    words=Vocab()\n",
        "    tags=Vocab()\n",
        "        \n",
        "    for sentence in train_data:\n",
        "        for word, tag in sentence:\n",
        "            words.add(word)\n",
        "            tags.add(tag)\n",
        "            \n",
        "    model = BiLSTMTagger(words, tags, embedding_dim=128, hidden_dim=256).to(device)\n",
        "    model.fit(train_data, lr=0.01, epochs=5)\n",
        "    torch.save({\n",
        "\t \t'model_state_dict': model.state_dict(),\n",
        "\t \t'words': model.words,\n",
        "\t \t'tags': model.tags,\n",
        "\t \t'embedding_dim': 128,\n",
        "\t \t'hidden_dim': 256\n",
        "\t }, 'bilstm_model.pth')\n",
        "    \n",
        "    val_acc = model.evaluate(val_data)\n",
        "    test_acc = model.evaluate(test_data)\n",
        "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    \n",
        "    for i, sentence in enumerate(test_data[:10]):\n",
        "        words = [w.lower() for w, _ in sentence]\n",
        "        word_idxs = [model.words.numberize(w) for w in words]\n",
        "        inputs = torch.tensor(word_idxs, dtype=torch.long).to(next(model.parameters()).device)\n",
        "        true_tags = [t for _, t in sentence]  # get actual tags\n",
        "        predicted_tags = model.predict(model(inputs))\n",
        "        predicted_tag_strs = [model.tags.denumberize(tag) for tag in predicted_tags]\n",
        "\n",
        "        print(f\"\\nSentence {i+1}:\")\n",
        "        for word, true_tag, predicted_tag in zip(words, true_tags, predicted_tag_strs):\n",
        "            print(f\"{word} {true_tag} {predicted_tag}\")\n",
        "\n",
        "main()\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-Z64TFuHqYc"
      },
      "source": [
        "**[2 points]** Free response: For the first 10 sentences of test.pos, **report the POS tags predicted by your model.**\n",
        "\n",
        "Sentence 1:\n",
        "the DT DT\n",
        "flight NN NN\n",
        "should MD MD\n",
        "arrive VB VB\n",
        "at IN IN\n",
        "eleven CD CD\n",
        "a.m RB RB\n",
        "tomorrow NN NN\n",
        ". PUNC PUNC\n",
        "\n",
        "Sentence 2:\n",
        "i PRP PRP\n",
        "would MD MD\n",
        "like VB VB\n",
        "to TO TO\n",
        "find VB VB\n",
        "a DT NNP\n",
        "flight NN NN\n",
        "that WDT WDT\n",
        "goes VBZ VBZ\n",
        "from IN IN\n",
        "la NNP NNP\n",
        "guardia NNP NNP\n",
        "airport NN NNP\n",
        "to TO TO\n",
        "san NNP NNP\n",
        "jose NNP NNP\n",
        ". PUNC PUNC\n",
        "\n",
        "Sentence 3:\n",
        "show VB VB\n",
        "me PRP PRP\n",
        "the DT DT\n",
        "flights NNS NNS\n",
        "from IN IN\n",
        "newark NNP NNP\n",
        "to TO TO\n",
        "los NNP NNP\n",
        "angeles NNP NNP\n",
        ". PUNC PUNC\n",
        "\n",
        "Sentence 4:\n",
        "what WDT WP\n",
        "airline NN NN\n",
        "is VBZ VBZ\n",
        "this DT DT\n",
        "? PUNC PUNC\n",
        "\n",
        "Sentence 5:\n",
        "show VB VB\n",
        "me PRP PRP\n",
        "the DT DT\n",
        "t NNP NNP\n",
        "w NNP NNP\n",
        "a NNP NNP\n",
        "flight NN NN\n",
        ". PUNC PUNC\n",
        "\n",
        "Sentence 6:\n",
        "i PRP PRP\n",
        "would MD MD\n",
        "like VB VB\n",
        "to TO TO\n",
        "travel VB VB\n",
        "to TO TO\n",
        "westchester NNP NNP\n",
        ". PUNC PUNC\n",
        "\n",
        "Sentence 7:\n",
        "list VB VB\n",
        "american NNP NNP\n",
        "airlines NNP NNP\n",
        "flights NNS NNS\n",
        "from IN IN\n",
        "new NNP NNP\n",
        "york NNP NNP\n",
        "newark NNP NNP\n",
        "to TO TO\n",
        "nashville NNP NNP\n",
        ". PUNC PUNC\n",
        "\n",
        "Sentence 8:\n",
        "thanks UH UH\n",
        ". PUNC PUNC\n",
        "\n",
        "Sentence 9:\n",
        "what WDT WP\n",
        "flights NNS NNS\n",
        "are VBP VBP\n",
        "there EX EX\n",
        "from IN IN\n",
        "nashville NNP NNP\n",
        "to TO TO\n",
        "houston NNP NNP\n",
        "tomorrow NN NN\n",
        "evening NN NN\n",
        "that WDT WDT\n",
        "serve VBP VBP\n",
        "dinner NN NN\n",
        "? PUNC PUNC\n",
        "\n",
        "Sentence 10:\n",
        "i PRP PRP\n",
        "'d MD MD\n",
        "like VB VB\n",
        "to TO TO\n",
        "fly VB VB\n",
        "next JJ JJ\n",
        "friday NNP NNP\n",
        ". PUNC PUNC\n",
        "\n",
        "- What works well, and what doesn't?\n",
        "> Words that have definitive purposes like punctuation, proper nouns, and verbs almost always get the right tags. The incorrectly tagged words are the ones where there's more ambiguity like nouns and proper nouns\n",
        "- For words tagged incorrectly, why do you think it happens, and what tag do they tend to get?\n",
        "> This can happen when words can have different parts of speech based on context. Airport for example can be a common noun (the airport) or a proper noun (JFK Airport), so it can be easy for the parser to get them mixed up\n",
        "- Think about micro- and macro-level tags (e.g., is it tagging NNS as NN, or VBD as NN, and which one is worse?).\n",
        "> It's okay when there are micro level tag issues because the general meaning of the sentence won't be skewed too much, but when you have macro level discrepancies, meanings can become misunderstood\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRcIs3YjIGf9"
      },
      "source": [
        "## Part 2: Probabilistic Context Free Grammar\n",
        "In this part, you will learn a PCFG given training data annotated with parse trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0I5vMzJ4MGbn"
      },
      "outputs": [],
      "source": [
        "import trees\n",
        "import fileinput\n",
        "import collections\n",
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "\"\"\"You should not need any other imports, but you may import anything that helps.\"\"\"\n",
        "\n",
        "counts = collections.defaultdict(collections.Counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx8PLiCSK0Jr"
      },
      "source": [
        "**[4 points]** Write code to read in trees and to count all the rules used in each tree. **Terminal nodes should be POS tags rather than words** to allow us to use the POS tagger. This means the grammar must contain a set of rules mapping each nonterminal POS tags in the PTB tag set to a terminal POS tag:\n",
        "```\n",
        "DT -> DT_t\n",
        "NN -> NN_t\n",
        "NNS -> NNS_t\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QReJeAGHLMYz"
      },
      "outputs": [],
      "source": [
        "def count_rules():\n",
        "\n",
        "    counts=defaultdict(Counter)  \n",
        "    probs = defaultdict(dict)\n",
        "    cfg = defaultdict(list)  \n",
        "    \n",
        "    with open(\"data/train.trees\", \"r\") as f:\n",
        "        for line in f:\n",
        "            tree = trees.Tree.from_str(line.strip())\n",
        "            if tree.root is None:\n",
        "                continue\n",
        "            \n",
        "            for node in tree.bottomup():\n",
        "                if len(node.children)>0:\n",
        "                    if len(node.children[0].children)==0:\n",
        "                        label=node.label\n",
        "                        if \"_\" in label:\n",
        "                            label = label.split(\"_\")[-1]\n",
        "                        label+=\"_t\"\n",
        "                        rhs = (label, )\n",
        "                    else:\n",
        "                        rhs=tuple(child.label for child in node.children)\n",
        "                        \n",
        "                    counts[node.label][rhs]+=1\n",
        "                    \n",
        "    for lhs in counts:\n",
        "        total = sum(counts[lhs].values())\n",
        "        for rhs in counts[lhs]:\n",
        "            probs[lhs][rhs] = counts[lhs][rhs]/total\n",
        "            cfg[rhs].append(lhs)\n",
        "                    \n",
        "           \n",
        "    return counts, probs, cfg         \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqcVdullK89D"
      },
      "source": [
        "**[3 points]** Write code to compute the conditional probability of each rule and **print the PCFG in a readable format**, such as:\n",
        "```\n",
        "NP -> DT NN # 0.5\n",
        "NP -> DT NNS # 0.5\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dv8X83skLM-U"
      },
      "outputs": [],
      "source": [
        "def print_cfg(counts, probs, cfg):\n",
        "    for rhs, parents in cfg.items():\n",
        "        for A in parents:\n",
        "            prob = probs[A].get(rhs)\n",
        "            count = counts[A].get(rhs)\n",
        "            if len(rhs) == 1:\n",
        "                print(f\"  {A} -> {rhs[0]} \"\n",
        "                      f\"[count={count}, prob={prob:.4f}]\")\n",
        "            elif len(rhs) == 2:\n",
        "                B, C = rhs\n",
        "                print(f\"  {A} -> ({B}, {C}) \"\n",
        "                      f\"[count={count}, prob={prob:.4f}]\")\n",
        "            else:\n",
        "                print(f\"  {A} -> {' '.join(rhs)} \"\n",
        "                      f\"[count={count}, prob={prob:.4f}]\")\n",
        "                \n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwlFtXGULE_2"
      },
      "source": [
        "**[3 points]** Running the code on train.trees, **report**:\n",
        "- How many unique rules are there?\n",
        "> 419\n",
        "- What are the top five most frequent rules, and how many times did each occur?\n",
        "> 1.\tIN --> IN_t\t(Occurence: 482 times)\n",
        "  2.\tPUNC --> PUNC_t\t(Occurence: 469 times)\n",
        "  3.\tNP_NNP --> NNP_t(Occurence: 451 times)\n",
        "  4.\tNNP --> NNP_t\t(Occurence: 408 times)\n",
        "  5.\tNN --> NN_t\t(Occurence: 281 times)\n",
        "- What are the top five highest-probability rules with left-hand side NP, and what are their probabilities?\n",
        "> 1.\tNP --> NNP NNP\t(Probability: 0.1928)\n",
        "  2.\tNP --> NP NP*\t(Probability: 0.1159)\n",
        "  3.\tNP --> DT NN\t(Probability: 0.1014)\n",
        "  4.\tNP --> DT NNS\t(Probability: 0.0855)\n",
        "  5.\tNP --> DT NP*\t(Probability: 0.0841)\n",
        "- **Free Response**: Did the most frequent rules surprise you? Why or why not?\n",
        "> Not really, the most frequent rules would be things like Punctuation, Proper nouns, and other nouns are the most common parts of speech in sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lbEBpSxPKwMa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VB -> VB_t # 1.0\n",
            "DT -> DT_t # 1.0\n",
            "NNS -> NNS_t # 1.0\n",
            "NP -> DT NNS # 0.08550724637681159\n",
            "NP -> NP NP* # 0.11594202898550725\n",
            "NP -> DT NN # 0.10144927536231885\n",
            "NP -> NP PP # 0.04057971014492753\n",
            "NP -> CD NNS # 0.002898550724637681\n",
            "NP -> NNP NNP # 0.1927536231884058\n",
            "NP -> NP_NNP NP # 0.013043478260869565\n",
            "NP -> NP SBAR # 0.01884057971014493\n",
            "NP -> NP_CD PP # 0.0014492753623188406\n",
            "NP -> DT JJ # 0.007246376811594203\n",
            "NP -> NNP NP* # 0.0463768115942029\n",
            "NP -> QP RB # 0.004347826086956522\n",
            "NP -> JJ NNP # 0.004347826086956522\n",
            "NP -> DT NP* # 0.08405797101449275\n",
            "NP -> CD RB # 0.030434782608695653\n",
            "NP -> PDT NP* # 0.004347826086956522\n",
            "NP -> NP VP # 0.005797101449275362\n",
            "NP -> CD NP* # 0.02318840579710145\n",
            "NP -> DT NX # 0.0014492753623188406\n",
            "NP -> NN NN # 0.013043478260869565\n",
            "NP -> NP_NNP NP_NNP # 0.007246376811594203\n",
            "NP -> NN NP* # 0.017391304347826087\n",
            "NP -> NNP JJ # 0.002898550724637681\n",
            "NP -> NNP POS # 0.002898550724637681\n",
            "NP -> NN NNS # 0.010144927536231883\n",
            "NP -> NP_NN NP* # 0.007246376811594203\n",
            "NP -> NP_NN PP # 0.0014492753623188406\n",
            "NP -> JJ NN # 0.015942028985507246\n",
            "NP -> CD CD # 0.002898550724637681\n",
            "NP -> NNP NN # 0.017391304347826087\n",
            "NP -> CD NN # 0.014492753623188406\n",
            "NP -> QP NN # 0.002898550724637681\n",
            "NP -> JJ NNS # 0.007246376811594203\n",
            "NP -> NP_NNP NP* # 0.008695652173913044\n",
            "NP -> NP ADJP_JJ # 0.0014492753623188406\n",
            "NP -> NP_DT PP # 0.002898550724637681\n",
            "NP -> NP_NNS NP* # 0.01884057971014493\n",
            "NP -> JJ NP* # 0.013043478260869565\n",
            "NP -> NP SBAR_S_VP # 0.0014492753623188406\n",
            "NP -> NP_NNS PP # 0.004347826086956522\n",
            "NP -> QP NNS # 0.010144927536231883\n",
            "NP -> NP NNS # 0.0014492753623188406\n",
            "NP -> NP_NNS VP # 0.002898550724637681\n",
            "NP -> JJS NP* # 0.005797101449275362\n",
            "NP -> WDT NNS # 0.0014492753623188406\n",
            "NP -> JJS NN # 0.011594202898550725\n",
            "NP -> CC NP* # 0.0014492753623188406\n",
            "NP -> NP NP # 0.002898550724637681\n",
            "NP -> NNP NNS # 0.0014492753623188406\n",
            "NP -> QP NP* # 0.0014492753623188406\n",
            "NP -> RB NP* # 0.0014492753623188406\n",
            "IN -> IN_t # 1.0\n",
            "NP_NNP -> NNP_t # 1.0\n",
            "PP -> IN NP_NNP # 0.34992679355783307\n",
            "PP -> TO NP_NNP # 0.232796486090776\n",
            "PP -> IN NP # 0.2884333821376281\n",
            "PP -> IN NP_PRP # 0.0014641288433382138\n",
            "PP -> TO NP # 0.0849194729136164\n",
            "PP -> IN NP_DT # 0.016105417276720352\n",
            "PP -> IN NP_NN # 0.013177159590043924\n",
            "PP -> IN ADJP_JJ # 0.0014641288433382138\n",
            "PP -> IN PP* # 0.0014641288433382138\n",
            "PP -> PP PP* # 0.0014641288433382138\n",
            "PP -> NP PP* # 0.0014641288433382138\n",
            "PP -> IN NP_NNPS # 0.0014641288433382138\n",
            "PP -> IN NP_NNS # 0.004392386530014641\n",
            "PP -> TO INTJ_UH # 0.0014641288433382138\n",
            "TO -> TO_t # 1.0\n",
            "WHNP_WDT -> WDT_t # 1.0\n",
            "VBP -> VBP_t # 1.0\n",
            "S_VP -> VBP PP # 0.014388489208633094\n",
            "S_VP -> VB NP # 0.1223021582733813\n",
            "S_VP -> TO VP # 0.1510791366906475\n",
            "S_VP -> MD VP # 0.014388489208633094\n",
            "S_VP -> VBZ VP* # 0.014388489208633094\n",
            "S_VP -> VB VP* # 0.5539568345323741\n",
            "S_VP -> VB NP_PRP # 0.014388489208633094\n",
            "S_VP -> VBP NP # 0.014388489208633094\n",
            "S_VP -> VBZ PP # 0.02158273381294964\n",
            "S_VP -> VBP NP_NN # 0.014388489208633094\n",
            "S_VP -> VBZ NP # 0.007194244604316547\n",
            "S_VP -> VB NP_NNS # 0.014388489208633094\n",
            "S_VP -> VBZ ADVP_RB # 0.007194244604316547\n",
            "S_VP -> VBP ADVP # 0.007194244604316547\n",
            "S_VP -> VP VP* # 0.014388489208633094\n",
            "S_VP -> VBP VP* # 0.007194244604316547\n",
            "S_VP -> VBZ NP_NN # 0.007194244604316547\n",
            "SBAR -> WHNP_WDT S_VP # 0.9090909090909091\n",
            "SBAR -> WHNP_WDT S # 0.045454545454545456\n",
            "SBAR -> WHNP SQ # 0.045454545454545456\n",
            "NP* -> PP SBAR # 0.011210762331838564\n",
            "NP* -> PP NP* # 0.1210762331838565\n",
            "NP* -> VP VP # 0.002242152466367713\n",
            "NP* -> VP SBAR # 0.004484304932735426\n",
            "NP* -> NNP NNP # 0.06053811659192825\n",
            "NP* -> PP PP # 0.242152466367713\n",
            "NP* -> PP NP # 0.013452914798206279\n",
            "NP* -> NN NN # 0.05605381165919283\n",
            "NP* -> JJ NP* # 0.02914798206278027\n",
            "NP* -> NNP NN # 0.006726457399103139\n",
            "NP* -> DT NNS # 0.006726457399103139\n",
            "NP* -> NN NP* # 0.02242152466367713\n",
            "NP* -> CD RB # 0.006726457399103139\n",
            "NP* -> CC NP* # 0.008968609865470852\n",
            "NP* -> JJ NN # 0.02242152466367713\n",
            "NP* -> PP NP_NN # 0.006726457399103139\n",
            "NP* -> PP VP # 0.02242152466367713\n",
            "NP* -> CD NN # 0.006726457399103139\n",
            "NP* -> CD NP* # 0.04484304932735426\n",
            "NP* -> NNP NP* # 0.0515695067264574\n",
            "NP* -> CD CD # 0.03811659192825112\n",
            "NP* -> NP_NN NP* # 0.004484304932735426\n",
            "NP* -> JJS NN # 0.01569506726457399\n",
            "NP* -> CC NP # 0.033632286995515695\n",
            "NP* -> X_TO PP # 0.002242152466367713\n",
            "NP* -> NP NP* # 0.004484304932735426\n",
            "NP* -> NN NNS # 0.0515695067264574\n",
            "NP* -> ADVP_RB NP # 0.004484304932735426\n",
            "NP* -> JJS NP* # 0.006726457399103139\n",
            "NP* -> SYM CD # 0.002242152466367713\n",
            "NP* -> SBAR PP # 0.002242152466367713\n",
            "NP* -> X_JJ NNS # 0.002242152466367713\n",
            "NP* -> NNS PP # 0.002242152466367713\n",
            "NP* -> CC NP_NNP # 0.008968609865470852\n",
            "NP* -> VP ADVP # 0.006726457399103139\n",
            "NP* -> CD NNS # 0.006726457399103139\n",
            "NP* -> NNP NNS # 0.011210762331838564\n",
            "NP* -> CC NNP # 0.006726457399103139\n",
            "NP* -> NN SYM # 0.004484304932735426\n",
            "NP* -> CC NP_NN # 0.002242152466367713\n",
            "NP* -> ADJP NN # 0.002242152466367713\n",
            "NP* -> RB RB # 0.008968609865470852\n",
            "NP* -> PP NP_NNS # 0.002242152466367713\n",
            "NP* -> NP ADVP_RB # 0.002242152466367713\n",
            "NP* -> PP ADVP_RB # 0.002242152466367713\n",
            "NP* -> PP ADJP # 0.002242152466367713\n",
            "NP* -> NNS QP # 0.002242152466367713\n",
            "NP* -> JJS NNS # 0.002242152466367713\n",
            "NP* -> JJ NNS # 0.002242152466367713\n",
            "NP* -> DT NP* # 0.002242152466367713\n",
            "NP* -> SYM NP* # 0.006726457399103139\n",
            "PUNC -> PUNC_t # 1.0\n",
            "TOP -> S_VP PUNC # 0.208955223880597\n",
            "TOP -> SQ PUNC # 0.04477611940298507\n",
            "TOP -> S PUNC # 0.11513859275053305\n",
            "TOP -> SBARQ PUNC # 0.21748400852878466\n",
            "TOP -> FRAG PUNC # 0.05970149253731343\n",
            "TOP -> FRAG_NP PUNC # 0.22388059701492538\n",
            "TOP -> INTJ_UH PUNC # 0.0042643923240938165\n",
            "TOP -> FRAG_NP_NN PUNC # 0.0042643923240938165\n",
            "TOP -> X_SBARQ PUNC # 0.0021321961620469083\n",
            "TOP -> SBAR PUNC # 0.0021321961620469083\n",
            "TOP -> FRAG_VP PUNC # 0.017057569296375266\n",
            "TOP -> NP PUNC # 0.006396588486140725\n",
            "TOP -> FRAG_NP_NNP PUNC # 0.0021321961620469083\n",
            "TOP -> FRAG_WHNP PUNC # 0.07036247334754797\n",
            "TOP -> FRAG_PP PUNC # 0.010660980810234541\n",
            "TOP -> FRAG_ADJP_JJ PUNC # 0.0042643923240938165\n",
            "TOP -> ADJP_JJ PUNC # 0.0021321961620469083\n",
            "TOP -> FRAG_ADJP_JJS PUNC # 0.0021321961620469083\n",
            "TOP -> X_S_VP PUNC # 0.0021321961620469083\n",
            "VBZ -> VBZ_t # 1.0\n",
            "NN -> NN_t # 1.0\n",
            "NP_NN -> NN_t # 1.0\n",
            "VP -> VB NP_NN # 0.044585987261146494\n",
            "VP -> VBG PP # 0.09554140127388536\n",
            "VP -> VBG NP # 0.03821656050955414\n",
            "VP -> VBP NP # 0.10828025477707007\n",
            "VP -> VB S # 0.012738853503184714\n",
            "VP -> VBP S_VP # 0.050955414012738856\n",
            "VP -> VB VP* # 0.12738853503184713\n",
            "VP -> VB S_VP # 0.08280254777070063\n",
            "VP -> VBZ PP # 0.012738853503184714\n",
            "VP -> MD VP # 0.12101910828025478\n",
            "VP -> VB PP # 0.044585987261146494\n",
            "VP -> VB NP # 0.09554140127388536\n",
            "VP -> VP VP* # 0.01910828025477707\n",
            "VP -> TO VP # 0.006369426751592357\n",
            "VP -> VBG NP_NNP # 0.006369426751592357\n",
            "VP -> VBP PP # 0.03184713375796178\n",
            "VP -> VBN PP # 0.01910828025477707\n",
            "VP -> VBP ADJP # 0.006369426751592357\n",
            "VP -> VB NP_NNP # 0.012738853503184714\n",
            "VP -> VBD NP # 0.006369426751592357\n",
            "VP -> VBP FRAG # 0.006369426751592357\n",
            "VP -> VBP VP* # 0.012738853503184714\n",
            "VP -> VB NP_NNS # 0.006369426751592357\n",
            "VP -> VBG VP* # 0.025477707006369428\n",
            "VP -> VBZ NP # 0.006369426751592357\n",
            "SQ* -> NP VP # 0.1323529411764706\n",
            "SQ* -> NP VP_VB # 0.058823529411764705\n",
            "SQ* -> NP_PRP VP # 0.08823529411764706\n",
            "SQ* -> NP_NN VP # 0.014705882352941176\n",
            "SQ* -> NP PP # 0.014705882352941176\n",
            "SQ* -> NP_NP_EX NP # 0.029411764705882353\n",
            "SQ* -> PP PP # 0.25\n",
            "SQ* -> NP SQ* # 0.029411764705882353\n",
            "SQ* -> NP_NP_EX SQ* # 0.25\n",
            "SQ* -> NP ADJP_JJ # 0.014705882352941176\n",
            "SQ* -> VBZ SQ* # 0.014705882352941176\n",
            "SQ* -> NP NP # 0.029411764705882353\n",
            "SQ* -> NP_DT NP # 0.014705882352941176\n",
            "SQ* -> PP SQ* # 0.029411764705882353\n",
            "SQ* -> NP_PRP VP_VB # 0.014705882352941176\n",
            "SQ* -> MD SQ* # 0.014705882352941176\n",
            "SQ -> VBZ SQ* # 0.17333333333333334\n",
            "SQ -> MD SQ* # 0.05333333333333334\n",
            "SQ -> VBP NP # 0.06666666666666667\n",
            "SQ -> VBP SQ* # 0.3466666666666667\n",
            "SQ -> VBZ NP_PRP # 0.02666666666666667\n",
            "SQ -> VBZ NP # 0.29333333333333333\n",
            "SQ -> X SQ* # 0.013333333333333334\n",
            "SQ -> VBP NP_NP_EX # 0.013333333333333334\n",
            "SQ -> INTJ_UH SQ* # 0.013333333333333334\n",
            "NP_PRP -> PRP_t # 1.0\n",
            "VBG -> VBG_t # 1.0\n",
            "S -> NP_PRP VP # 0.5901639344262295\n",
            "S -> NP_NN VP_VBN # 0.01639344262295082\n",
            "S -> NP VP # 0.13114754098360656\n",
            "S -> S S* # 0.03278688524590164\n",
            "S -> INTJ_UH VP # 0.06557377049180328\n",
            "S -> INTJ_UH S* # 0.01639344262295082\n",
            "S -> NP_NNP VP_VBZ # 0.01639344262295082\n",
            "S -> PP S* # 0.03278688524590164\n",
            "S -> NP_PRP S* # 0.01639344262295082\n",
            "S -> ADVP_RB S* # 0.03278688524590164\n",
            "S -> ADVP_RB VP # 0.03278688524590164\n",
            "S -> NP_NN VP # 0.01639344262295082\n",
            "VP_VBN -> VBN_t # 1.0\n",
            "CD -> CD_t # 1.0\n",
            "MD -> MD_t # 1.0\n",
            "ADVP_RB -> RB_t # 1.0\n",
            "NNP -> NNP_t # 1.0\n",
            "ADVP -> ADVP_RB PP # 0.2857142857142857\n",
            "ADVP -> RB PP # 0.14285714285714285\n",
            "ADVP -> NP RB # 0.5714285714285714\n",
            "VP* -> PP ADVP # 0.0064516129032258064\n",
            "VP* -> NP_PRP VP* # 0.01935483870967742\n",
            "VP* -> NP_PRP ADVP # 0.0064516129032258064\n",
            "VP* -> PP NP_NN # 0.0064516129032258064\n",
            "VP* -> PP PP # 0.21935483870967742\n",
            "VP* -> CC VP # 0.04516129032258064\n",
            "VP* -> PRT_RP NP # 0.0064516129032258064\n",
            "VP* -> NP_PRP NP # 0.4967741935483871\n",
            "VP* -> NP_PRP PP # 0.0064516129032258064\n",
            "VP* -> X_TO PP # 0.0064516129032258064\n",
            "VP* -> PP VP* # 0.07096774193548387\n",
            "VP* -> ADVP NP # 0.0064516129032258064\n",
            "VP* -> PP ADJP # 0.0064516129032258064\n",
            "VP* -> ADVP_RB VP* # 0.0064516129032258064\n",
            "VP* -> PP NP # 0.012903225806451613\n",
            "VP* -> NP INTJ_UH # 0.012903225806451613\n",
            "VP* -> NP_NNP PP # 0.03225806451612903\n",
            "VP* -> NP_NNP VP* # 0.012903225806451613\n",
            "VP* -> NP PP # 0.012903225806451613\n",
            "VP* -> RB VP # 0.0064516129032258064\n",
            "NP_CD -> CD_t # 1.0\n",
            "CC -> CC_t # 1.0\n",
            "JJ -> JJ_t # 1.0\n",
            "S* -> CC S # 0.25\n",
            "S* -> NP_PRP VP # 0.625\n",
            "S* -> ADVP_RB VP # 0.125\n",
            "RB -> RB_t # 1.0\n",
            "QP -> RB CD # 0.21428571428571427\n",
            "QP -> IN QP* # 0.14285714285714285\n",
            "QP -> RBR QP* # 0.2857142857142857\n",
            "QP -> CD QP* # 0.21428571428571427\n",
            "QP -> CC JJR # 0.07142857142857142\n",
            "QP -> RB QP* # 0.07142857142857142\n",
            "WRB -> WRB_t # 1.0\n",
            "WHNP -> WRB RB # 0.054945054945054944\n",
            "WHNP -> WHNP ADJP_JJ # 0.01098901098901099\n",
            "WHNP -> WHNP_WDT PP # 0.18681318681318682\n",
            "WHNP -> WDT NN # 0.03296703296703297\n",
            "WHNP -> WDT NNS # 0.6703296703296703\n",
            "WHNP -> WHNP PP # 0.03296703296703297\n",
            "WHNP -> WHADJP NNS # 0.01098901098901099\n",
            "ADJP_JJ -> JJ_t # 1.0\n",
            "VP_VB -> VB_t # 1.0\n",
            "SBARQ -> WHNP SQ # 0.0784313725490196\n",
            "SBARQ -> PP SBARQ* # 0.00980392156862745\n",
            "SBARQ -> WHNP_WP SQ # 0.19607843137254902\n",
            "SBARQ -> WHNP SQ_VP # 0.4117647058823529\n",
            "SBARQ -> WHNP_WHNP SQ # 0.18627450980392157\n",
            "SBARQ -> WHNP_WDT SQ # 0.049019607843137254\n",
            "SBARQ -> WHADVP_WRB SQ # 0.00980392156862745\n",
            "SBARQ -> WHNP_WDT SQ_VP # 0.058823529411764705\n",
            "PRT_RP -> RP_t # 1.0\n",
            "PDT -> PDT_t # 1.0\n",
            "WDT -> WDT_t # 1.0\n",
            "WHNP_WHNP -> WDT NNS # 0.9\n",
            "WHNP_WHNP -> WHADJP NNS # 0.1\n",
            "SQ_VP -> VBP PP # 0.14\n",
            "SQ_VP -> VP VP* # 0.04\n",
            "SQ_VP -> VBP VP # 0.02\n",
            "SQ_VP -> VBP NP # 0.08\n",
            "SQ_VP -> VBZ NP_NN # 0.02\n",
            "SQ_VP -> VBP NP_NN # 0.08\n",
            "SQ_VP -> VBP ADVP_RB # 0.02\n",
            "SQ_VP -> VBZ ADJP_JJ # 0.06\n",
            "SQ_VP -> VBP ADVP_RBS # 0.02\n",
            "SQ_VP -> VBZ ADVP_RBS # 0.02\n",
            "SQ_VP -> VBP VP* # 0.36\n",
            "SQ_VP -> VBP PP_IN # 0.02\n",
            "SQ_VP -> VBP ADJP_JJ # 0.02\n",
            "SQ_VP -> VBP NP_NNS # 0.02\n",
            "SQ_VP -> VBZ NP # 0.02\n",
            "SQ_VP -> VBP NP_NNP # 0.02\n",
            "SQ_VP -> VBZ ADJP_JJR # 0.04\n",
            "SBARQ* -> WHNP_WHNP SQ_VP # 1.0\n",
            "WHNP_WP -> WP_t # 1.0\n",
            "NP_DT -> DT_t # 1.0\n",
            "WP -> WP_t # 1.0\n",
            "X -> WP IN # 0.6666666666666666\n",
            "X -> WRB IN # 0.16666666666666666\n",
            "X -> VBZ NP_DT # 0.16666666666666666\n",
            "FRAG -> X PP # 0.06896551724137931\n",
            "FRAG -> NP INTJ_UH # 0.034482758620689655\n",
            "FRAG -> X NP # 0.06896551724137931\n",
            "FRAG -> NP FRAG* # 0.06896551724137931\n",
            "FRAG -> NP_NNP FRAG* # 0.10344827586206896\n",
            "FRAG -> PP VP # 0.034482758620689655\n",
            "FRAG -> ADVP_RB NP_NNP # 0.034482758620689655\n",
            "FRAG -> NP_NNP PP # 0.1724137931034483\n",
            "FRAG -> PP PP # 0.13793103448275862\n",
            "FRAG -> PP FRAG* # 0.034482758620689655\n",
            "FRAG -> WHNP PP # 0.034482758620689655\n",
            "FRAG -> WHNP FRAG* # 0.06896551724137931\n",
            "FRAG -> NP_NN FRAG* # 0.034482758620689655\n",
            "FRAG -> X NP_NNP # 0.034482758620689655\n",
            "FRAG -> NP NP # 0.06896551724137931\n",
            "VBN -> VBN_t # 1.0\n",
            "ADJP -> JJ PP # 0.2\n",
            "ADJP -> RB ADJP* # 0.2\n",
            "ADJP -> RBS JJ # 0.2\n",
            "ADJP -> JJR PP # 0.4\n",
            "NX_NN -> NN_t # 1.0\n",
            "NX -> NN NN # 0.5\n",
            "NX -> NX_NN NX* # 0.5\n",
            "NX* -> CC NX # 1.0\n",
            "X_TO -> TO_t # 1.0\n",
            "WHADVP_WRB -> WRB_t # 1.0\n",
            "INTJ_UH -> UH_t # 1.0\n",
            "FRAG_NP -> NP PP # 0.05714285714285714\n",
            "FRAG_NP -> NP NP* # 0.21904761904761905\n",
            "FRAG_NP -> NP_NNS NP* # 0.3333333333333333\n",
            "FRAG_NP -> JJS NN # 0.0380952380952381\n",
            "FRAG_NP -> NP VP_VBG # 0.009523809523809525\n",
            "FRAG_NP -> NNP NN # 0.01904761904761905\n",
            "FRAG_NP -> NP_NNS PP # 0.06666666666666667\n",
            "FRAG_NP -> NN NNS # 0.009523809523809525\n",
            "FRAG_NP -> NP_NN PP # 0.05714285714285714\n",
            "FRAG_NP -> NP_NN NP* # 0.0380952380952381\n",
            "FRAG_NP -> JJ NP* # 0.009523809523809525\n",
            "FRAG_NP -> DT NNS # 0.009523809523809525\n",
            "FRAG_NP -> NN NP* # 0.0380952380952381\n",
            "FRAG_NP -> CD NNS # 0.009523809523809525\n",
            "FRAG_NP -> JJ NNS # 0.009523809523809525\n",
            "FRAG_NP -> NP_NNP NP* # 0.009523809523809525\n",
            "FRAG_NP -> JJS NP* # 0.01904761904761905\n",
            "FRAG_NP -> NN NN # 0.02857142857142857\n",
            "FRAG_NP -> NP_NNS ADJP # 0.009523809523809525\n",
            "FRAG_NP -> JJS NNS # 0.009523809523809525\n",
            "JJS -> JJS_t # 1.0\n",
            "NP_NP_EX -> EX_t # 1.0\n",
            "POS -> POS_t # 1.0\n",
            "VP_VBZ -> VBZ_t # 1.0\n",
            "VBD -> VBD_t # 1.0\n",
            "ADJP* -> RB PP # 1.0\n",
            "FRAG* -> NP NP # 0.08333333333333333\n",
            "FRAG* -> NP FRAG* # 0.16666666666666666\n",
            "FRAG* -> PP PP # 0.4166666666666667\n",
            "FRAG* -> PP VP # 0.08333333333333333\n",
            "FRAG* -> PP NP # 0.08333333333333333\n",
            "FRAG* -> PP FRAG* # 0.08333333333333333\n",
            "FRAG* -> PP ADVP_RB # 0.08333333333333333\n",
            "FRAG_NP_NN -> NN_t # 1.0\n",
            "ADVP_RBS -> RBS_t # 1.0\n",
            "WHADJP -> WRB JJ # 1.0\n",
            "PP_IN -> IN_t # 1.0\n",
            "X_SBARQ -> WHNP SQ_VP # 1.0\n",
            "SYM -> SYM_t # 1.0\n",
            "QP* -> JJS CD # 0.14285714285714285\n",
            "QP* -> IN CD # 0.2857142857142857\n",
            "QP* -> CD CD # 0.2857142857142857\n",
            "QP* -> CD QP* # 0.14285714285714285\n",
            "QP* -> IN QP* # 0.07142857142857142\n",
            "QP* -> JJR QP* # 0.07142857142857142\n",
            "X_JJ -> JJ_t # 1.0\n",
            "NP_NNS -> NNS_t # 1.0\n",
            "FRAG_VP -> VBG VP* # 0.75\n",
            "FRAG_VP -> VBG PP # 0.125\n",
            "FRAG_VP -> VBG NP # 0.125\n",
            "VP_VBG -> VBG_t # 1.0\n",
            "PP* -> IN NP # 0.25\n",
            "PP* -> CC PP* # 0.25\n",
            "PP* -> CC PP # 0.25\n",
            "PP* -> IN S_VP_VBG # 0.25\n",
            "SBAR_S_VP -> TO VP_VB # 1.0\n",
            "RBR -> RBR_t # 1.0\n",
            "FRAG_NP_NNP -> NNP_t # 1.0\n",
            "WHNP* -> PP PP # 0.5614035087719298\n",
            "WHNP* -> PP WHNP* # 0.43859649122807015\n",
            "FRAG_WHNP -> WHNP WHNP* # 0.9393939393939394\n",
            "FRAG_WHNP -> NP WHNP* # 0.030303030303030304\n",
            "FRAG_WHNP -> WHNP PP # 0.030303030303030304\n",
            "FRAG_PP -> IN NP_NNP # 0.2\n",
            "FRAG_PP -> IN NP # 0.8\n",
            "S_VP_VBG -> VBG_t # 1.0\n",
            "ADJP_JJR -> JJR_t # 1.0\n",
            "FRAG_ADJP_JJ -> JJ_t # 1.0\n",
            "RBS -> RBS_t # 1.0\n",
            "NP_NNPS -> NNPS_t # 1.0\n",
            "FRAG_ADJP_JJS -> JJS_t # 1.0\n",
            "JJR -> JJR_t # 1.0\n",
            "X_S_VP -> VB VP* # 1.0\n",
            "Unique rules: 419\n",
            "CFG: \n",
            "  VB -> VB_t [count=166, prob=1.0000]\n",
            "  VP_VB -> VB_t [count=6, prob=1.0000]\n",
            "  DT -> DT_t [count=198, prob=1.0000]\n",
            "  NP_DT -> DT_t [count=15, prob=1.0000]\n",
            "  NNS -> NNS_t [count=209, prob=1.0000]\n",
            "  NP_NNS -> NNS_t [count=69, prob=1.0000]\n",
            "  NP -> (DT, NNS) [count=59, prob=0.0855]\n",
            "  NP* -> (DT, NNS) [count=3, prob=0.0067]\n",
            "  FRAG_NP -> (DT, NNS) [count=1, prob=0.0095]\n",
            "  NP -> (NP, NP*) [count=80, prob=0.1159]\n",
            "  NP* -> (NP, NP*) [count=2, prob=0.0045]\n",
            "  FRAG_NP -> (NP, NP*) [count=23, prob=0.2190]\n",
            "  NP -> (DT, NN) [count=70, prob=0.1014]\n",
            "  NP -> (NP, PP) [count=28, prob=0.0406]\n",
            "  SQ* -> (NP, PP) [count=1, prob=0.0147]\n",
            "  VP* -> (NP, PP) [count=2, prob=0.0129]\n",
            "  FRAG_NP -> (NP, PP) [count=6, prob=0.0571]\n",
            "  NP -> (CD, NNS) [count=2, prob=0.0029]\n",
            "  NP* -> (CD, NNS) [count=3, prob=0.0067]\n",
            "  FRAG_NP -> (CD, NNS) [count=1, prob=0.0095]\n",
            "  NP -> (NNP, NNP) [count=133, prob=0.1928]\n",
            "  NP* -> (NNP, NNP) [count=27, prob=0.0605]\n",
            "  NP -> (NP_NNP, NP) [count=9, prob=0.0130]\n",
            "  NP -> (NP, SBAR) [count=13, prob=0.0188]\n",
            "  NP -> (NP_CD, PP) [count=1, prob=0.0014]\n",
            "  NP -> (DT, JJ) [count=5, prob=0.0072]\n",
            "  NP -> (NNP, NP*) [count=32, prob=0.0464]\n",
            "  NP* -> (NNP, NP*) [count=23, prob=0.0516]\n",
            "  NP -> (QP, RB) [count=3, prob=0.0043]\n",
            "  NP -> (JJ, NNP) [count=3, prob=0.0043]\n",
            "  NP -> (DT, NP*) [count=58, prob=0.0841]\n",
            "  NP* -> (DT, NP*) [count=1, prob=0.0022]\n",
            "  NP -> (CD, RB) [count=21, prob=0.0304]\n",
            "  NP* -> (CD, RB) [count=3, prob=0.0067]\n",
            "  NP -> (PDT, NP*) [count=3, prob=0.0043]\n",
            "  NP -> (NP, VP) [count=4, prob=0.0058]\n",
            "  SQ* -> (NP, VP) [count=9, prob=0.1324]\n",
            "  S -> (NP, VP) [count=8, prob=0.1311]\n",
            "  NP -> (CD, NP*) [count=16, prob=0.0232]\n",
            "  NP* -> (CD, NP*) [count=20, prob=0.0448]\n",
            "  NP -> (DT, NX) [count=1, prob=0.0014]\n",
            "  NP -> (NN, NN) [count=9, prob=0.0130]\n",
            "  NP* -> (NN, NN) [count=25, prob=0.0561]\n",
            "  NX -> (NN, NN) [count=1, prob=0.5000]\n",
            "  FRAG_NP -> (NN, NN) [count=3, prob=0.0286]\n",
            "  NP -> (NP_NNP, NP_NNP) [count=5, prob=0.0072]\n",
            "  NP -> (NN, NP*) [count=12, prob=0.0174]\n",
            "  NP* -> (NN, NP*) [count=10, prob=0.0224]\n",
            "  FRAG_NP -> (NN, NP*) [count=4, prob=0.0381]\n",
            "  NP -> (NNP, JJ) [count=2, prob=0.0029]\n",
            "  NP -> (NNP, POS) [count=2, prob=0.0029]\n",
            "  NP -> (NN, NNS) [count=7, prob=0.0101]\n",
            "  NP* -> (NN, NNS) [count=23, prob=0.0516]\n",
            "  FRAG_NP -> (NN, NNS) [count=1, prob=0.0095]\n",
            "  NP -> (NP_NN, NP*) [count=5, prob=0.0072]\n",
            "  NP* -> (NP_NN, NP*) [count=2, prob=0.0045]\n",
            "  FRAG_NP -> (NP_NN, NP*) [count=4, prob=0.0381]\n",
            "  NP -> (NP_NN, PP) [count=1, prob=0.0014]\n",
            "  FRAG_NP -> (NP_NN, PP) [count=6, prob=0.0571]\n",
            "  NP -> (JJ, NN) [count=11, prob=0.0159]\n",
            "  NP* -> (JJ, NN) [count=10, prob=0.0224]\n",
            "  NP -> (CD, CD) [count=2, prob=0.0029]\n",
            "  NP* -> (CD, CD) [count=17, prob=0.0381]\n",
            "  QP* -> (CD, CD) [count=4, prob=0.2857]\n",
            "  NP -> (NNP, NN) [count=12, prob=0.0174]\n",
            "  NP* -> (NNP, NN) [count=3, prob=0.0067]\n",
            "  FRAG_NP -> (NNP, NN) [count=2, prob=0.0190]\n",
            "  NP -> (CD, NN) [count=10, prob=0.0145]\n",
            "  NP* -> (CD, NN) [count=3, prob=0.0067]\n",
            "  NP -> (QP, NN) [count=2, prob=0.0029]\n",
            "  NP -> (JJ, NNS) [count=5, prob=0.0072]\n",
            "  NP* -> (JJ, NNS) [count=1, prob=0.0022]\n",
            "  FRAG_NP -> (JJ, NNS) [count=1, prob=0.0095]\n",
            "  NP -> (NP_NNP, NP*) [count=6, prob=0.0087]\n",
            "  FRAG_NP -> (NP_NNP, NP*) [count=1, prob=0.0095]\n",
            "  NP -> (NP, ADJP_JJ) [count=1, prob=0.0014]\n",
            "  SQ* -> (NP, ADJP_JJ) [count=1, prob=0.0147]\n",
            "  NP -> (NP_DT, PP) [count=2, prob=0.0029]\n",
            "  NP -> (NP_NNS, NP*) [count=13, prob=0.0188]\n",
            "  FRAG_NP -> (NP_NNS, NP*) [count=35, prob=0.3333]\n",
            "  NP -> (JJ, NP*) [count=9, prob=0.0130]\n",
            "  NP* -> (JJ, NP*) [count=13, prob=0.0291]\n",
            "  FRAG_NP -> (JJ, NP*) [count=1, prob=0.0095]\n",
            "  NP -> (NP, SBAR_S_VP) [count=1, prob=0.0014]\n",
            "  NP -> (NP_NNS, PP) [count=3, prob=0.0043]\n",
            "  FRAG_NP -> (NP_NNS, PP) [count=7, prob=0.0667]\n",
            "  NP -> (QP, NNS) [count=7, prob=0.0101]\n",
            "  NP -> (NP, NNS) [count=1, prob=0.0014]\n",
            "  NP -> (NP_NNS, VP) [count=2, prob=0.0029]\n",
            "  NP -> (JJS, NP*) [count=4, prob=0.0058]\n",
            "  NP* -> (JJS, NP*) [count=3, prob=0.0067]\n",
            "  FRAG_NP -> (JJS, NP*) [count=2, prob=0.0190]\n",
            "  NP -> (WDT, NNS) [count=1, prob=0.0014]\n",
            "  WHNP -> (WDT, NNS) [count=61, prob=0.6703]\n",
            "  WHNP_WHNP -> (WDT, NNS) [count=18, prob=0.9000]\n",
            "  NP -> (JJS, NN) [count=8, prob=0.0116]\n",
            "  NP* -> (JJS, NN) [count=7, prob=0.0157]\n",
            "  FRAG_NP -> (JJS, NN) [count=4, prob=0.0381]\n",
            "  NP -> (CC, NP*) [count=1, prob=0.0014]\n",
            "  NP* -> (CC, NP*) [count=4, prob=0.0090]\n",
            "  NP -> (NP, NP) [count=2, prob=0.0029]\n",
            "  SQ* -> (NP, NP) [count=2, prob=0.0294]\n",
            "  FRAG -> (NP, NP) [count=2, prob=0.0690]\n",
            "  FRAG* -> (NP, NP) [count=1, prob=0.0833]\n",
            "  NP -> (NNP, NNS) [count=1, prob=0.0014]\n",
            "  NP* -> (NNP, NNS) [count=5, prob=0.0112]\n",
            "  NP -> (QP, NP*) [count=1, prob=0.0014]\n",
            "  NP -> (RB, NP*) [count=1, prob=0.0014]\n",
            "  IN -> IN_t [count=482, prob=1.0000]\n",
            "  PP_IN -> IN_t [count=1, prob=1.0000]\n",
            "  NP_NNP -> NNP_t [count=451, prob=1.0000]\n",
            "  NNP -> NNP_t [count=408, prob=1.0000]\n",
            "  FRAG_NP_NNP -> NNP_t [count=1, prob=1.0000]\n",
            "  PP -> (IN, NP_NNP) [count=239, prob=0.3499]\n",
            "  FRAG_PP -> (IN, NP_NNP) [count=1, prob=0.2000]\n",
            "  PP -> (TO, NP_NNP) [count=159, prob=0.2328]\n",
            "  PP -> (IN, NP) [count=197, prob=0.2884]\n",
            "  PP* -> (IN, NP) [count=1, prob=0.2500]\n",
            "  FRAG_PP -> (IN, NP) [count=4, prob=0.8000]\n",
            "  PP -> (IN, NP_PRP) [count=1, prob=0.0015]\n",
            "  PP -> (TO, NP) [count=58, prob=0.0849]\n",
            "  PP -> (IN, NP_DT) [count=11, prob=0.0161]\n",
            "  PP -> (IN, NP_NN) [count=9, prob=0.0132]\n",
            "  PP -> (IN, ADJP_JJ) [count=1, prob=0.0015]\n",
            "  PP -> (IN, PP*) [count=1, prob=0.0015]\n",
            "  PP -> (PP, PP*) [count=1, prob=0.0015]\n",
            "  PP -> (NP, PP*) [count=1, prob=0.0015]\n",
            "  PP -> (IN, NP_NNPS) [count=1, prob=0.0015]\n",
            "  PP -> (IN, NP_NNS) [count=3, prob=0.0044]\n",
            "  PP -> (TO, INTJ_UH) [count=1, prob=0.0015]\n",
            "  TO -> TO_t [count=241, prob=1.0000]\n",
            "  X_TO -> TO_t [count=2, prob=1.0000]\n",
            "  WHNP_WDT -> WDT_t [count=49, prob=1.0000]\n",
            "  WDT -> WDT_t [count=83, prob=1.0000]\n",
            "  VBP -> VBP_t [count=114, prob=1.0000]\n",
            "  S_VP -> (VBP, PP) [count=2, prob=0.0144]\n",
            "  VP -> (VBP, PP) [count=5, prob=0.0318]\n",
            "  SQ_VP -> (VBP, PP) [count=7, prob=0.1400]\n",
            "  S_VP -> (VB, NP) [count=17, prob=0.1223]\n",
            "  VP -> (VB, NP) [count=15, prob=0.0955]\n",
            "  S_VP -> (TO, VP) [count=21, prob=0.1511]\n",
            "  VP -> (TO, VP) [count=1, prob=0.0064]\n",
            "  S_VP -> (MD, VP) [count=2, prob=0.0144]\n",
            "  VP -> (MD, VP) [count=19, prob=0.1210]\n",
            "  S_VP -> (VBZ, VP*) [count=2, prob=0.0144]\n",
            "  S_VP -> (VB, VP*) [count=77, prob=0.5540]\n",
            "  VP -> (VB, VP*) [count=20, prob=0.1274]\n",
            "  X_S_VP -> (VB, VP*) [count=1, prob=1.0000]\n",
            "  S_VP -> (VB, NP_PRP) [count=2, prob=0.0144]\n",
            "  S_VP -> (VBP, NP) [count=2, prob=0.0144]\n",
            "  VP -> (VBP, NP) [count=17, prob=0.1083]\n",
            "  SQ -> (VBP, NP) [count=5, prob=0.0667]\n",
            "  SQ_VP -> (VBP, NP) [count=4, prob=0.0800]\n",
            "  S_VP -> (VBZ, PP) [count=3, prob=0.0216]\n",
            "  VP -> (VBZ, PP) [count=2, prob=0.0127]\n",
            "  S_VP -> (VBP, NP_NN) [count=2, prob=0.0144]\n",
            "  SQ_VP -> (VBP, NP_NN) [count=4, prob=0.0800]\n",
            "  S_VP -> (VBZ, NP) [count=1, prob=0.0072]\n",
            "  VP -> (VBZ, NP) [count=1, prob=0.0064]\n",
            "  SQ -> (VBZ, NP) [count=22, prob=0.2933]\n",
            "  SQ_VP -> (VBZ, NP) [count=1, prob=0.0200]\n",
            "  S_VP -> (VB, NP_NNS) [count=2, prob=0.0144]\n",
            "  VP -> (VB, NP_NNS) [count=1, prob=0.0064]\n",
            "  S_VP -> (VBZ, ADVP_RB) [count=1, prob=0.0072]\n",
            "  S_VP -> (VBP, ADVP) [count=1, prob=0.0072]\n",
            "  S_VP -> (VP, VP*) [count=2, prob=0.0144]\n",
            "  VP -> (VP, VP*) [count=3, prob=0.0191]\n",
            "  SQ_VP -> (VP, VP*) [count=2, prob=0.0400]\n",
            "  S_VP -> (VBP, VP*) [count=1, prob=0.0072]\n",
            "  VP -> (VBP, VP*) [count=2, prob=0.0127]\n",
            "  SQ_VP -> (VBP, VP*) [count=18, prob=0.3600]\n",
            "  S_VP -> (VBZ, NP_NN) [count=1, prob=0.0072]\n",
            "  SQ_VP -> (VBZ, NP_NN) [count=1, prob=0.0200]\n",
            "  SBAR -> (WHNP_WDT, S_VP) [count=20, prob=0.9091]\n",
            "  SBAR -> (WHNP_WDT, S) [count=1, prob=0.0455]\n",
            "  SBAR -> (WHNP, SQ) [count=1, prob=0.0455]\n",
            "  SBARQ -> (WHNP, SQ) [count=8, prob=0.0784]\n",
            "  NP* -> (PP, SBAR) [count=5, prob=0.0112]\n",
            "  NP* -> (PP, NP*) [count=54, prob=0.1211]\n",
            "  NP* -> (VP, VP) [count=1, prob=0.0022]\n",
            "  NP* -> (VP, SBAR) [count=2, prob=0.0045]\n",
            "  NP* -> (PP, PP) [count=108, prob=0.2422]\n",
            "  SQ* -> (PP, PP) [count=17, prob=0.2500]\n",
            "  VP* -> (PP, PP) [count=34, prob=0.2194]\n",
            "  FRAG -> (PP, PP) [count=4, prob=0.1379]\n",
            "  FRAG* -> (PP, PP) [count=5, prob=0.4167]\n",
            "  WHNP* -> (PP, PP) [count=32, prob=0.5614]\n",
            "  NP* -> (PP, NP) [count=6, prob=0.0135]\n",
            "  VP* -> (PP, NP) [count=2, prob=0.0129]\n",
            "  FRAG* -> (PP, NP) [count=1, prob=0.0833]\n",
            "  NP* -> (PP, NP_NN) [count=3, prob=0.0067]\n",
            "  VP* -> (PP, NP_NN) [count=1, prob=0.0065]\n",
            "  NP* -> (PP, VP) [count=10, prob=0.0224]\n",
            "  FRAG -> (PP, VP) [count=1, prob=0.0345]\n",
            "  FRAG* -> (PP, VP) [count=1, prob=0.0833]\n",
            "  NP* -> (CC, NP) [count=15, prob=0.0336]\n",
            "  NP* -> (X_TO, PP) [count=1, prob=0.0022]\n",
            "  VP* -> (X_TO, PP) [count=1, prob=0.0065]\n",
            "  NP* -> (ADVP_RB, NP) [count=2, prob=0.0045]\n",
            "  NP* -> (SYM, CD) [count=1, prob=0.0022]\n",
            "  NP* -> (SBAR, PP) [count=1, prob=0.0022]\n",
            "  NP* -> (X_JJ, NNS) [count=1, prob=0.0022]\n",
            "  NP* -> (NNS, PP) [count=1, prob=0.0022]\n",
            "  NP* -> (CC, NP_NNP) [count=4, prob=0.0090]\n",
            "  NP* -> (VP, ADVP) [count=3, prob=0.0067]\n",
            "  NP* -> (CC, NNP) [count=3, prob=0.0067]\n",
            "  NP* -> (NN, SYM) [count=2, prob=0.0045]\n",
            "  NP* -> (CC, NP_NN) [count=1, prob=0.0022]\n",
            "  NP* -> (ADJP, NN) [count=1, prob=0.0022]\n",
            "  NP* -> (RB, RB) [count=4, prob=0.0090]\n",
            "  NP* -> (PP, NP_NNS) [count=1, prob=0.0022]\n",
            "  NP* -> (NP, ADVP_RB) [count=1, prob=0.0022]\n",
            "  NP* -> (PP, ADVP_RB) [count=1, prob=0.0022]\n",
            "  FRAG* -> (PP, ADVP_RB) [count=1, prob=0.0833]\n",
            "  NP* -> (PP, ADJP) [count=1, prob=0.0022]\n",
            "  VP* -> (PP, ADJP) [count=1, prob=0.0065]\n",
            "  NP* -> (NNS, QP) [count=1, prob=0.0022]\n",
            "  NP* -> (JJS, NNS) [count=1, prob=0.0022]\n",
            "  FRAG_NP -> (JJS, NNS) [count=1, prob=0.0095]\n",
            "  NP* -> (SYM, NP*) [count=3, prob=0.0067]\n",
            "  PUNC -> PUNC_t [count=469, prob=1.0000]\n",
            "  TOP -> (S_VP, PUNC) [count=98, prob=0.2090]\n",
            "  TOP -> (SQ, PUNC) [count=21, prob=0.0448]\n",
            "  TOP -> (S, PUNC) [count=54, prob=0.1151]\n",
            "  TOP -> (SBARQ, PUNC) [count=102, prob=0.2175]\n",
            "  TOP -> (FRAG, PUNC) [count=28, prob=0.0597]\n",
            "  TOP -> (FRAG_NP, PUNC) [count=105, prob=0.2239]\n",
            "  TOP -> (INTJ_UH, PUNC) [count=2, prob=0.0043]\n",
            "  TOP -> (FRAG_NP_NN, PUNC) [count=2, prob=0.0043]\n",
            "  TOP -> (X_SBARQ, PUNC) [count=1, prob=0.0021]\n",
            "  TOP -> (SBAR, PUNC) [count=1, prob=0.0021]\n",
            "  TOP -> (FRAG_VP, PUNC) [count=8, prob=0.0171]\n",
            "  TOP -> (NP, PUNC) [count=3, prob=0.0064]\n",
            "  TOP -> (FRAG_NP_NNP, PUNC) [count=1, prob=0.0021]\n",
            "  TOP -> (FRAG_WHNP, PUNC) [count=33, prob=0.0704]\n",
            "  TOP -> (FRAG_PP, PUNC) [count=5, prob=0.0107]\n",
            "  TOP -> (FRAG_ADJP_JJ, PUNC) [count=2, prob=0.0043]\n",
            "  TOP -> (ADJP_JJ, PUNC) [count=1, prob=0.0021]\n",
            "  TOP -> (FRAG_ADJP_JJS, PUNC) [count=1, prob=0.0021]\n",
            "  TOP -> (X_S_VP, PUNC) [count=1, prob=0.0021]\n",
            "  VBZ -> VBZ_t [count=58, prob=1.0000]\n",
            "  VP_VBZ -> VBZ_t [count=1, prob=1.0000]\n",
            "  NN -> NN_t [count=281, prob=1.0000]\n",
            "  NP_NN -> NN_t [count=51, prob=1.0000]\n",
            "  NX_NN -> NN_t [count=1, prob=1.0000]\n",
            "  FRAG_NP_NN -> NN_t [count=2, prob=1.0000]\n",
            "  VP -> (VB, NP_NN) [count=7, prob=0.0446]\n",
            "  VP -> (VBG, PP) [count=15, prob=0.0955]\n",
            "  FRAG_VP -> (VBG, PP) [count=1, prob=0.1250]\n",
            "  VP -> (VBG, NP) [count=6, prob=0.0382]\n",
            "  FRAG_VP -> (VBG, NP) [count=1, prob=0.1250]\n",
            "  VP -> (VB, S) [count=2, prob=0.0127]\n",
            "  VP -> (VBP, S_VP) [count=8, prob=0.0510]\n",
            "  VP -> (VB, S_VP) [count=13, prob=0.0828]\n",
            "  VP -> (VB, PP) [count=7, prob=0.0446]\n",
            "  VP -> (VBG, NP_NNP) [count=1, prob=0.0064]\n",
            "  VP -> (VBN, PP) [count=3, prob=0.0191]\n",
            "  VP -> (VBP, ADJP) [count=1, prob=0.0064]\n",
            "  VP -> (VB, NP_NNP) [count=2, prob=0.0127]\n",
            "  VP -> (VBD, NP) [count=1, prob=0.0064]\n",
            "  VP -> (VBP, FRAG) [count=1, prob=0.0064]\n",
            "  VP -> (VBG, VP*) [count=4, prob=0.0255]\n",
            "  FRAG_VP -> (VBG, VP*) [count=6, prob=0.7500]\n",
            "  SQ* -> (NP, VP_VB) [count=4, prob=0.0588]\n",
            "  SQ* -> (NP_PRP, VP) [count=6, prob=0.0882]\n",
            "  S -> (NP_PRP, VP) [count=36, prob=0.5902]\n",
            "  S* -> (NP_PRP, VP) [count=5, prob=0.6250]\n",
            "  SQ* -> (NP_NN, VP) [count=1, prob=0.0147]\n",
            "  S -> (NP_NN, VP) [count=1, prob=0.0164]\n",
            "  SQ* -> (NP_NP_EX, NP) [count=2, prob=0.0294]\n",
            "  SQ* -> (NP, SQ*) [count=2, prob=0.0294]\n",
            "  SQ* -> (NP_NP_EX, SQ*) [count=17, prob=0.2500]\n",
            "  SQ* -> (VBZ, SQ*) [count=1, prob=0.0147]\n",
            "  SQ -> (VBZ, SQ*) [count=13, prob=0.1733]\n",
            "  SQ* -> (NP_DT, NP) [count=1, prob=0.0147]\n",
            "  SQ* -> (PP, SQ*) [count=2, prob=0.0294]\n",
            "  SQ* -> (NP_PRP, VP_VB) [count=1, prob=0.0147]\n",
            "  SQ* -> (MD, SQ*) [count=1, prob=0.0147]\n",
            "  SQ -> (MD, SQ*) [count=4, prob=0.0533]\n",
            "  SQ -> (VBP, SQ*) [count=26, prob=0.3467]\n",
            "  SQ -> (VBZ, NP_PRP) [count=2, prob=0.0267]\n",
            "  SQ -> (X, SQ*) [count=1, prob=0.0133]\n",
            "  SQ -> (VBP, NP_NP_EX) [count=1, prob=0.0133]\n",
            "  SQ -> (INTJ_UH, SQ*) [count=1, prob=0.0133]\n",
            "  NP_PRP -> PRP_t [count=136, prob=1.0000]\n",
            "  VBG -> VBG_t [count=34, prob=1.0000]\n",
            "  VP_VBG -> VBG_t [count=1, prob=1.0000]\n",
            "  S_VP_VBG -> VBG_t [count=1, prob=1.0000]\n",
            "  S -> (NP_NN, VP_VBN) [count=1, prob=0.0164]\n",
            "  S -> (S, S*) [count=2, prob=0.0328]\n",
            "  S -> (INTJ_UH, VP) [count=4, prob=0.0656]\n",
            "  S -> (INTJ_UH, S*) [count=1, prob=0.0164]\n",
            "  S -> (NP_NNP, VP_VBZ) [count=1, prob=0.0164]\n",
            "  S -> (PP, S*) [count=2, prob=0.0328]\n",
            "  S -> (NP_PRP, S*) [count=1, prob=0.0164]\n",
            "  S -> (ADVP_RB, S*) [count=2, prob=0.0328]\n",
            "  S -> (ADVP_RB, VP) [count=2, prob=0.0328]\n",
            "  S* -> (ADVP_RB, VP) [count=1, prob=0.1250]\n",
            "  VP_VBN -> VBN_t [count=1, prob=1.0000]\n",
            "  VBN -> VBN_t [count=3, prob=1.0000]\n",
            "  CD -> CD_t [count=140, prob=1.0000]\n",
            "  NP_CD -> CD_t [count=1, prob=1.0000]\n",
            "  MD -> MD_t [count=26, prob=1.0000]\n",
            "  ADVP_RB -> RB_t [count=16, prob=1.0000]\n",
            "  RB -> RB_t [count=53, prob=1.0000]\n",
            "  ADVP -> (ADVP_RB, PP) [count=2, prob=0.2857]\n",
            "  ADVP -> (RB, PP) [count=1, prob=0.1429]\n",
            "  ADJP* -> (RB, PP) [count=1, prob=1.0000]\n",
            "  ADVP -> (NP, RB) [count=4, prob=0.5714]\n",
            "  VP* -> (PP, ADVP) [count=1, prob=0.0065]\n",
            "  VP* -> (NP_PRP, VP*) [count=3, prob=0.0194]\n",
            "  VP* -> (NP_PRP, ADVP) [count=1, prob=0.0065]\n",
            "  VP* -> (CC, VP) [count=7, prob=0.0452]\n",
            "  VP* -> (PRT_RP, NP) [count=1, prob=0.0065]\n",
            "  VP* -> (NP_PRP, NP) [count=77, prob=0.4968]\n",
            "  VP* -> (NP_PRP, PP) [count=1, prob=0.0065]\n",
            "  VP* -> (PP, VP*) [count=11, prob=0.0710]\n",
            "  VP* -> (ADVP, NP) [count=1, prob=0.0065]\n",
            "  VP* -> (ADVP_RB, VP*) [count=1, prob=0.0065]\n",
            "  VP* -> (NP, INTJ_UH) [count=2, prob=0.0129]\n",
            "  FRAG -> (NP, INTJ_UH) [count=1, prob=0.0345]\n",
            "  VP* -> (NP_NNP, PP) [count=5, prob=0.0323]\n",
            "  FRAG -> (NP_NNP, PP) [count=5, prob=0.1724]\n",
            "  VP* -> (NP_NNP, VP*) [count=2, prob=0.0129]\n",
            "  VP* -> (RB, VP) [count=1, prob=0.0065]\n",
            "  CC -> CC_t [count=41, prob=1.0000]\n",
            "  JJ -> JJ_t [count=66, prob=1.0000]\n",
            "  ADJP_JJ -> JJ_t [count=9, prob=1.0000]\n",
            "  X_JJ -> JJ_t [count=1, prob=1.0000]\n",
            "  FRAG_ADJP_JJ -> JJ_t [count=2, prob=1.0000]\n",
            "  S* -> (CC, S) [count=2, prob=0.2500]\n",
            "  QP -> (RB, CD) [count=3, prob=0.2143]\n",
            "  QP -> (IN, QP*) [count=2, prob=0.1429]\n",
            "  QP* -> (IN, QP*) [count=1, prob=0.0714]\n",
            "  QP -> (RBR, QP*) [count=4, prob=0.2857]\n",
            "  QP -> (CD, QP*) [count=3, prob=0.2143]\n",
            "  QP* -> (CD, QP*) [count=2, prob=0.1429]\n",
            "  QP -> (CC, JJR) [count=1, prob=0.0714]\n",
            "  QP -> (RB, QP*) [count=1, prob=0.0714]\n",
            "  WRB -> WRB_t [count=9, prob=1.0000]\n",
            "  WHADVP_WRB -> WRB_t [count=1, prob=1.0000]\n",
            "  WHNP -> (WRB, RB) [count=5, prob=0.0549]\n",
            "  WHNP -> (WHNP, ADJP_JJ) [count=1, prob=0.0110]\n",
            "  WHNP -> (WHNP_WDT, PP) [count=17, prob=0.1868]\n",
            "  WHNP -> (WDT, NN) [count=3, prob=0.0330]\n",
            "  WHNP -> (WHNP, PP) [count=3, prob=0.0330]\n",
            "  FRAG -> (WHNP, PP) [count=1, prob=0.0345]\n",
            "  FRAG_WHNP -> (WHNP, PP) [count=1, prob=0.0303]\n",
            "  WHNP -> (WHADJP, NNS) [count=1, prob=0.0110]\n",
            "  WHNP_WHNP -> (WHADJP, NNS) [count=2, prob=0.1000]\n",
            "  SBARQ -> (PP, SBARQ*) [count=1, prob=0.0098]\n",
            "  SBARQ -> (WHNP_WP, SQ) [count=20, prob=0.1961]\n",
            "  SBARQ -> (WHNP, SQ_VP) [count=42, prob=0.4118]\n",
            "  X_SBARQ -> (WHNP, SQ_VP) [count=1, prob=1.0000]\n",
            "  SBARQ -> (WHNP_WHNP, SQ) [count=19, prob=0.1863]\n",
            "  SBARQ -> (WHNP_WDT, SQ) [count=5, prob=0.0490]\n",
            "  SBARQ -> (WHADVP_WRB, SQ) [count=1, prob=0.0098]\n",
            "  SBARQ -> (WHNP_WDT, SQ_VP) [count=6, prob=0.0588]\n",
            "  PRT_RP -> RP_t [count=1, prob=1.0000]\n",
            "  PDT -> PDT_t [count=3, prob=1.0000]\n",
            "  SQ_VP -> (VBP, VP) [count=1, prob=0.0200]\n",
            "  SQ_VP -> (VBP, ADVP_RB) [count=1, prob=0.0200]\n",
            "  SQ_VP -> (VBZ, ADJP_JJ) [count=3, prob=0.0600]\n",
            "  SQ_VP -> (VBP, ADVP_RBS) [count=1, prob=0.0200]\n",
            "  SQ_VP -> (VBZ, ADVP_RBS) [count=1, prob=0.0200]\n",
            "  SQ_VP -> (VBP, PP_IN) [count=1, prob=0.0200]\n",
            "  SQ_VP -> (VBP, ADJP_JJ) [count=1, prob=0.0200]\n",
            "  SQ_VP -> (VBP, NP_NNS) [count=1, prob=0.0200]\n",
            "  SQ_VP -> (VBP, NP_NNP) [count=1, prob=0.0200]\n",
            "  SQ_VP -> (VBZ, ADJP_JJR) [count=2, prob=0.0400]\n",
            "  SBARQ* -> (WHNP_WHNP, SQ_VP) [count=1, prob=1.0000]\n",
            "  WHNP_WP -> WP_t [count=20, prob=1.0000]\n",
            "  WP -> WP_t [count=4, prob=1.0000]\n",
            "  X -> (WP, IN) [count=4, prob=0.6667]\n",
            "  X -> (WRB, IN) [count=1, prob=0.1667]\n",
            "  X -> (VBZ, NP_DT) [count=1, prob=0.1667]\n",
            "  FRAG -> (X, PP) [count=2, prob=0.0690]\n",
            "  FRAG -> (X, NP) [count=2, prob=0.0690]\n",
            "  FRAG -> (NP, FRAG*) [count=2, prob=0.0690]\n",
            "  FRAG* -> (NP, FRAG*) [count=2, prob=0.1667]\n",
            "  FRAG -> (NP_NNP, FRAG*) [count=3, prob=0.1034]\n",
            "  FRAG -> (ADVP_RB, NP_NNP) [count=1, prob=0.0345]\n",
            "  FRAG -> (PP, FRAG*) [count=1, prob=0.0345]\n",
            "  FRAG* -> (PP, FRAG*) [count=1, prob=0.0833]\n",
            "  FRAG -> (WHNP, FRAG*) [count=2, prob=0.0690]\n",
            "  FRAG -> (NP_NN, FRAG*) [count=1, prob=0.0345]\n",
            "  FRAG -> (X, NP_NNP) [count=1, prob=0.0345]\n",
            "  ADJP -> (JJ, PP) [count=1, prob=0.2000]\n",
            "  ADJP -> (RB, ADJP*) [count=1, prob=0.2000]\n",
            "  ADJP -> (RBS, JJ) [count=1, prob=0.2000]\n",
            "  ADJP -> (JJR, PP) [count=2, prob=0.4000]\n",
            "  NX -> (NX_NN, NX*) [count=1, prob=0.5000]\n",
            "  NX* -> (CC, NX) [count=1, prob=1.0000]\n",
            "  INTJ_UH -> UH_t [count=12, prob=1.0000]\n",
            "  FRAG_NP -> (NP, VP_VBG) [count=1, prob=0.0095]\n",
            "  FRAG_NP -> (NP_NNS, ADJP) [count=1, prob=0.0095]\n",
            "  JJS -> JJS_t [count=32, prob=1.0000]\n",
            "  FRAG_ADJP_JJS -> JJS_t [count=1, prob=1.0000]\n",
            "  NP_NP_EX -> EX_t [count=20, prob=1.0000]\n",
            "  POS -> POS_t [count=2, prob=1.0000]\n",
            "  VBD -> VBD_t [count=1, prob=1.0000]\n",
            "  ADVP_RBS -> RBS_t [count=2, prob=1.0000]\n",
            "  RBS -> RBS_t [count=1, prob=1.0000]\n",
            "  WHADJP -> (WRB, JJ) [count=3, prob=1.0000]\n",
            "  SYM -> SYM_t [count=6, prob=1.0000]\n",
            "  QP* -> (JJS, CD) [count=2, prob=0.1429]\n",
            "  QP* -> (IN, CD) [count=4, prob=0.2857]\n",
            "  QP* -> (JJR, QP*) [count=1, prob=0.0714]\n",
            "  PP* -> (CC, PP*) [count=1, prob=0.2500]\n",
            "  PP* -> (CC, PP) [count=1, prob=0.2500]\n",
            "  PP* -> (IN, S_VP_VBG) [count=1, prob=0.2500]\n",
            "  SBAR_S_VP -> (TO, VP_VB) [count=1, prob=1.0000]\n",
            "  RBR -> RBR_t [count=4, prob=1.0000]\n",
            "  WHNP* -> (PP, WHNP*) [count=25, prob=0.4386]\n",
            "  FRAG_WHNP -> (WHNP, WHNP*) [count=31, prob=0.9394]\n",
            "  FRAG_WHNP -> (NP, WHNP*) [count=1, prob=0.0303]\n",
            "  ADJP_JJR -> JJR_t [count=2, prob=1.0000]\n",
            "  JJR -> JJR_t [count=4, prob=1.0000]\n",
            "  NP_NNPS -> NNPS_t [count=1, prob=1.0000]\n",
            "\n",
            "Number of rules: 419\n",
            "Top five most frequent rules: \n",
            "1.\tIN --> IN_t\t(Occurence: 482 times)\n",
            "2.\tPUNC --> PUNC_t\t(Occurence: 469 times)\n",
            "3.\tNP_NNP --> NNP_t\t(Occurence: 451 times)\n",
            "4.\tNNP --> NNP_t\t(Occurence: 408 times)\n",
            "5.\tNN --> NN_t\t(Occurence: 281 times)\n",
            "\n",
            "Top five most frequent left-side NP rules:\n",
            "1.\tNP --> NNP NNP\t(Probability: 0.1928)\n",
            "2.\tNP --> NP NP*\t(Probability: 0.1159)\n",
            "3.\tNP --> DT NN\t(Probability: 0.1014)\n",
            "4.\tNP --> DT NNS\t(Probability: 0.0855)\n",
            "5.\tNP --> DT NP*\t(Probability: 0.0841)\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "  counts, probs, cfg = count_rules()\n",
        "  \n",
        "  num=0\n",
        "  for lhs in probs:\n",
        "    for rhs in probs[lhs]:\n",
        "      print(f\"{lhs} -> {\" \".join(rhs)} # {probs[lhs][rhs]}\")\n",
        "      num+=1\n",
        "      \n",
        "  print(f\"Unique rules: {num}\")\n",
        "  \n",
        "  print(\"CFG: \")\n",
        "  print_cfg(counts, probs, cfg)\n",
        "  \n",
        "  rules = []\n",
        "  for lhs in counts:\n",
        "    for rhs in counts[lhs]:\n",
        "      count = counts[lhs][rhs]\n",
        "      rule = f\"{lhs} --> {' '.join(rhs)}\"\n",
        "      rules.append((rule, count))\n",
        "      \n",
        "  rules = sorted(rules, key = lambda x:x[1], reverse=True)\n",
        "  \n",
        "  print(f\"Number of rules: {len(rules)}\")\n",
        "  print(\"Top five most frequent rules: \")\n",
        "  for r, (rule, count) in enumerate(rules[:5], 1):\n",
        "    print(f\"{r}.\\t{rule}\\t(Occurence: {count} times)\")\n",
        "  print()\n",
        "  \n",
        "  np_rules = list(sorted([(rule, prob) for rule, prob in probs['NP'].items()], key = lambda x:x[1], reverse=True))\n",
        "  print(\"Top five most frequent left-side NP rules:\")\n",
        "  for r, (rule, prob) in enumerate(np_rules[:5], 1):\n",
        "    print(f\"{r}.\\tNP --> {\" \".join(rule)}\\t(Probability: {prob:.4f})\")\n",
        "  \n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "main()\n",
        "      \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwt8mwdwNm62"
      },
      "source": [
        "# Part 3: CKY Parsing\n",
        "In this part, you will implement the CKY parsing algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PHqLIdiNuVu"
      },
      "source": [
        "**[4 points]** Implement a CKY parser that takes your grammar and a set of POS tags as input, and outputs the highest-probability parse. If you can't find any parse, output a blank line. Use **log-probabilities** to avoid underflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_5xDOfWuNz5k"
      },
      "outputs": [],
      "source": [
        "def parse_cky(sentences):\n",
        "    counts, probs, cfg = count_rules()\n",
        "\n",
        "    def reconstruct(label, i, k, words):\n",
        "        bp = backptr[(i, k)][label]\n",
        "        if isinstance(bp, str): \n",
        "            return f\"({label} {bp})\"\n",
        "        else:\n",
        "            (left_label, j, right_label) = bp\n",
        "            left_subtree  = reconstruct(left_label,  i, j, words)\n",
        "            right_subtree = reconstruct(right_label, j, k, words)\n",
        "            return f\"({label} {left_subtree} {right_subtree})\"\n",
        "\n",
        "    for index, sentence in enumerate(sentences):\n",
        "        chart = dict()\n",
        "        backptr = dict()\n",
        "\n",
        "        words, tags = zip(*sentence)\n",
        "        n = len(sentence)\n",
        "\n",
        "        for i, tag in enumerate(tags):\n",
        "            preterm = (tag + \"_t\",)  \n",
        "            chart[(i, i+1)] = {}\n",
        "            backptr[(i, i+1)] = {}\n",
        "            for lhs in cfg.get(preterm, []):  \n",
        "                chart[(i, i+1)][lhs] = math.log(probs[lhs][preterm])\n",
        "                backptr[(i, i+1)][lhs] = words[i]\n",
        "\n",
        "        for l in range(2, n+1):          \n",
        "            for i in range(n-l+1):      \n",
        "                k = i + l          \n",
        "                chart[(i, k)] = {}\n",
        "                backptr[(i, k)] = {}\n",
        "                for j in range(i+1, k): \n",
        "                    for left_label in chart[(i, j)]:\n",
        "                        for right_label in chart[(j, k)]:\n",
        "                            if (left_label, right_label) in cfg:\n",
        "                                for A in cfg[(left_label, right_label)]:\n",
        "                                    candidate_score = (chart[(i, j)][left_label] + chart[(j, k)][right_label] + math.log(probs[A][(left_label, right_label)]))\n",
        "                                    if A not in chart[(i, k)] or candidate_score > chart[(i, k)][A]:\n",
        "                                        chart[(i, k)][A] = candidate_score\n",
        "                                        backptr[(i, k)][A] = (left_label, j, right_label)\n",
        "\n",
        "        print(f\"Sentence {index+1}:\")\n",
        "        if \"TOP\" in chart[(0, n)]:\n",
        "            tree_str = reconstruct(\"TOP\", 0, n, words)\n",
        "            print(f\"{tree_str}\\tProbability: {chart[(0, n)][\"TOP\"]}\\n\") \n",
        "        else:\n",
        "            print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msTjbIrvO40p"
      },
      "source": [
        "**[1 point]** Test the CKY parser with the test set with **gold POS tags**. **Print** the parse trees for the **first 10 test sentences** (use empty line for no parse). For full credit, these should match or be similar to the true parses in test.trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VZo8CB5DO_NP"
      },
      "outputs": [],
      "source": [
        "def test_parser():\n",
        "  print(\"first 10 test sentences with gold POS tags\")\n",
        "  print()\n",
        "  sentences = read_pos_file(\"data/test.pos\")[0:10]\n",
        "  parse_cky(sentences)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rxWxJiEPHq6"
      },
      "source": [
        "**[2 points]** Integrate your **Bi-LSTM tagger** to assign POS tags to the words of any input sentence before passing it in to your CKY parser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MKrdEKBNQUy2"
      },
      "outputs": [],
      "source": [
        "def assign_tags():\n",
        "  checkpoint = torch.load('bilstm_model.pth', map_location=device, weights_only=False)\n",
        "  words = checkpoint['words']\n",
        "  tags = checkpoint['tags']\n",
        "  embedding_dim = checkpoint['embedding_dim']\n",
        "  hidden_dim = checkpoint['hidden_dim']\n",
        "  model = BiLSTMTagger(words, tags, embedding_dim, hidden_dim).to(device)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teR_j7R3Qn0W"
      },
      "source": [
        "**[1 point]** Test the full pipeline (**input → tagger → CKY**). **Print** the parse trees for the **first 10 test sentences** (use empty line for no parse) using the predicted POS tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BHxn4_yQQixu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first 10 test sentences with gold POS tags\n",
            "\n",
            "Sentence 1:\n",
            "(TOP (S (NP (DT the) (NN flight)) (VP (MD should) (VP (VB arrive) (VP* (PP (IN at) (NP (CD eleven) (RB a.m))) (NP_NN tomorrow))))) (PUNC .))\tProbability: -20.432453165513543\n",
            "\n",
            "Sentence 2:\n",
            "(TOP (S (NP_PRP i) (VP (MD would) (VP (VB like) (S_VP (TO to) (VP (VB find) (NP (NP (DT a) (NN flight)) (SBAR (WHNP_WDT that) (S_VP (VBZ goes) (VP* (PP (IN from) (NP (NNP la) (NP* (NNP guardia) (NN airport)))) (PP (TO to) (NP (NNP san) (NNP jose)))))))))))) (PUNC .))\tProbability: -37.07221194904133\n",
            "\n",
            "Sentence 3:\n",
            "(TOP (S_VP (VB show) (VP* (NP_PRP me) (NP (NP (DT the) (NNS flights)) (NP* (PP (IN from) (NP_NNP newark)) (PP (TO to) (NP (NNP los) (NNP angeles))))))) (PUNC .))\tProbability: -14.05035596365033\n",
            "\n",
            "Sentence 4:\n",
            "\n",
            "Sentence 5:\n",
            "(TOP (S_VP (VB show) (VP* (NP_PRP me) (NP (NP (DT the) (NP* (NNP t) (NNP w))) (NP* (NNP a) (NN flight))))) (PUNC .))\tProbability: -15.293025795448274\n",
            "\n",
            "Sentence 6:\n",
            "(TOP (S (NP_PRP i) (VP (MD would) (VP (VB like) (S_VP (TO to) (VP (VB travel) (PP (TO to) (NP_NNP westchester))))))) (PUNC .))\tProbability: -13.749954730718668\n",
            "\n",
            "Sentence 7:\n",
            "(TOP (S_VP (VB list) (NP (NP (NNP american) (NP* (NNP airlines) (NNS flights))) (NP* (PP (IN from) (NP (NNP new) (NP* (NNP york) (NNP newark)))) (PP (TO to) (NP_NNP nashville))))) (PUNC .))\tProbability: -23.377904870031664\n",
            "\n",
            "Sentence 8:\n",
            "(TOP (INTJ_UH thanks) (PUNC .))\tProbability: -5.457455587886334\n",
            "\n",
            "Sentence 9:\n",
            "(TOP (SBARQ (WHNP_WHNP (WDT what) (NNS flights)) (SQ (VBP are) (SQ* (NP_NP_EX there) (SQ* (PP (IN from) (NP_NNP nashville)) (PP (TO to) (NP (NP (NNP houston) (NP* (NN tomorrow) (NN evening))) (SBAR (WHNP_WDT that) (S_VP (VBP serve) (NP_NN dinner))))))))) (PUNC ?))\tProbability: -24.920365753804205\n",
            "\n",
            "Sentence 10:\n",
            "(TOP (S (NP_PRP i) (VP (MD 'd) (VP (VB like) (S_VP (TO to) (VP (VB fly) (NP (JJ next) (NNP friday))))))) (PUNC .))\tProbability: -16.968303330244407\n",
            "\n",
            "\n",
            "first 10 test sentences with predicted POS tags\n",
            "\n",
            "Sentence 1:\n",
            "(TOP (S (NP (DT the) (NN flight)) (VP (MD should) (VP (VBP arrive) (VP* (PP (IN at) (NP (CD eleven) (RB a.m))) (NP_NN tomorrow))))) (PUNC .))\tProbability: -22.735038258507586\n",
            "\n",
            "Sentence 2:\n",
            "(TOP (S (NP_PRP i) (VP (MD would) (VP (VB like) (S_VP (TO to) (VP (VB find) (NP (NP (DT a) (NN flight)) (SBAR (WHNP_WDT that) (S_VP (VBZ goes) (VP* (PP (IN from) (NP (NNP la) (NP* (NNP guardia) (NNP airport)))) (PP (TO to) (NP (NNP san) (NNP jose)))))))))))) (PUNC .))\tProbability: -34.87498737170512\n",
            "\n",
            "Sentence 3:\n",
            "(TOP (S_VP (VB show) (VP* (NP_PRP me) (NP (NP (DT the) (NNS flights)) (NP* (PP (IN from) (NP_NNP newark)) (PP (TO to) (NP (NNP los) (NNP angeles))))))) (PUNC .))\tProbability: -14.05035596365033\n",
            "\n",
            "Sentence 4:\n",
            "\n",
            "Sentence 5:\n",
            "(TOP (S_VP (VB show) (VP* (NP_PRP me) (NP (NP (DT the) (NP* (NNP t) (NNP w))) (NP (DT a) (NN flight))))) (PUNC .))\tProbability: -16.268394941752202\n",
            "\n",
            "Sentence 6:\n",
            "(TOP (S (NP_PRP i) (VP (MD would) (VP (VB like) (S_VP (TO to) (VP (VB travel) (PP (TO to) (NP_NNP westchester))))))) (PUNC .))\tProbability: -13.749954730718668\n",
            "\n",
            "Sentence 7:\n",
            "(TOP (S_VP (VB list) (NP (NP (NNP american) (NP* (NNP airlines) (NNS flights))) (NP* (PP (IN from) (NP (NNP new) (NP* (NNP york) (NNP newark)))) (PP (TO to) (NP_NNP nashville))))) (PUNC .))\tProbability: -23.377904870031664\n",
            "\n",
            "Sentence 8:\n",
            "(TOP (INTJ_UH thanks) (PUNC .))\tProbability: -5.457455587886334\n",
            "\n",
            "Sentence 9:\n",
            "(TOP (SBARQ (WHNP_WHNP (WDT what) (NNS flights)) (SQ (VBP are) (SQ* (NP_NP_EX there) (SQ* (PP (IN from) (NP_NNP nashville)) (PP (TO to) (NP (NP (NNP houston) (NP* (NN tomorrow) (NN evening))) (SBAR (WHNP_WDT that) (S_VP (VBP serve) (NP_NN dinner))))))))) (PUNC ?))\tProbability: -24.920365753804205\n",
            "\n",
            "Sentence 10:\n",
            "(TOP (S (NP_PRP i) (VP (MD 'd) (VP (VB like) (S_VP (TO to) (VP (VBP fly) (NP (JJ next) (NNP friday))))))) (PUNC .))\tProbability: -16.843140187290402\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<string>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ],
      "source": [
        "def test_pipeline():\n",
        "\n",
        "  test_parser()\n",
        "  \n",
        "  model = assign_tags()\n",
        "  print()\n",
        "  print(\"first 10 test sentences with predicted POS tags\")\n",
        "  print()\n",
        "  \n",
        "  sentences = read_pos_file(\"data/test.pos\")[:10]\n",
        "  sentences_with_predicted_tags = []\n",
        "  for sentence in sentences:\n",
        "    words, tags = zip(*sentence)\n",
        "    word_idxs = [model.words.numberize(word.lower()) for word in words]\n",
        "    sentence = torch.tensor(word_idxs, dtype=torch.long).to(device)\n",
        "    scores = model(sentence)\n",
        "    predicted = model.predict(scores)\n",
        "    predicted_tags = [model.tags.denumberize(tag) for tag in predicted]\n",
        "    sentences_with_predicted_tags.append(list(zip(words, predicted_tags)))\n",
        "\n",
        "  parse_cky(sentences_with_predicted_tags)\n",
        "\n",
        "test_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzmDfxlxQx1e"
      },
      "source": [
        "**[2 points]** Free response: For the first 10 sentences of test.pos…\n",
        "- Is there a difference in which sentences it fails to parse given gold tags vs. your tagger's outputs? Why or why not?\n",
        "> No, both of them were able to parse the same sentences. This is because the grammar rules are broad enough that both POS assignments can still fall under the cfg rules\n",
        "- Which ones does it do well on (i.e., match the true parse in test.trees), and why?\n",
        "> it did well on sentences that were short and had words with specific meanings like prepositions and proper nouns. This is because shorter words mean less context is needed to understand the sentence, and specific words reduce ambiguity, which makes it easier for the parser to capture the meaning/structure of the sentence\n",
        "- Which ones does it do poorly on (but still produces a parse), and why?\n",
        "> I'm noticing that the longer the sentence. is, there is usually less confidence, which I think is just do to added sentence structure complexity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0V01er4RDAh"
      },
      "source": [
        "**Congratulations! That's a wrap on HW2. Onto neural methods and greater generalizability.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
